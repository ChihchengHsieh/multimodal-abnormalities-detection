{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building\n",
    "\n",
    "In this notebook, we can build the model or load the pretrained model to understand what's the model we want and what's the input & label required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First test out how does the Faster R-CNN in torchvision work.\n",
    "\n",
    "- [Pytorch Tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) \n",
    "- [Pytorch Fast R-CNN implementation](https://pytorch.org/vision/stable/_modules/torchvision/models/detection/faster_rcnn.html)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model structure\n",
    "This is the pre-trained model we load; however, the we still don't know what're the required input and output. And, we gotta solve that thorugh official example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): Sequential(\n",
       "    (0): ConvNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): ConvNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(1280, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(1280, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=62720, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataset provided here, we can analyse the input and output for this model. The testing dataset (PennFundanPed) can be downloaded [here](https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip). After downloading, put it in the root of your working space. \n",
    "\n",
    "For the evaludation purpose, the official docuemnt recommend `pycocotools`. It can be installed through `pip install pycocotools`. (Note: For windows user, run `pip install git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI` instead). Before you install `pycocotools`, you may need to isntall cython through `pip install cython` or `conda install cython` if you're using conda for package mangement.\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/37566901/156862758-629a2d26-728f-4790-bd9f-bd71b310bc7b.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this has to be renamed.\n",
    "import utils.transforms as T\n",
    "\n",
    "# import torchvision.transforms as torch_transform\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_img, example_target = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 536, 559])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[258., 181., 400., 430.],\n",
       "         [ 25., 170., 140., 485.]]),\n",
       " 'labels': tensor([1, 1]),\n",
       " 'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       " 'image_id': tensor([0]),\n",
       " 'area': tensor([35358., 36225.]),\n",
       " 'iscrowd': tensor([0, 0])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mike8\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "# For Training\n",
    "images, targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "output = model(images, targets)  # Returns losses and detections\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)  # Returns predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[344.0933, 269.6074, 373.6343, 300.0000],\n",
       "          [  6.1380, 115.4398,  16.0083, 125.6891],\n",
       "          [192.9464, 263.3619, 228.0760, 299.9628],\n",
       "          [  0.0000,  93.9464,  17.5664, 157.3088],\n",
       "          [  5.1091, 137.5406,  15.0588, 147.9350],\n",
       "          [383.7705, 271.3126, 399.5110, 295.4370],\n",
       "          [395.0635, 271.2076, 399.9822, 298.4234],\n",
       "          [385.6763, 273.4329, 399.9025, 280.7289],\n",
       "          [362.1655, 280.1430, 378.8711, 289.1548],\n",
       "          [395.4576, 259.8518, 399.9704, 285.3814],\n",
       "          [376.2971, 264.8966, 399.8041, 300.0000],\n",
       "          [389.2911, 280.3214, 399.9664, 291.1273],\n",
       "          [366.3659, 294.7784, 381.6030, 300.0000],\n",
       "          [359.0735, 255.9739, 389.5890, 299.9889],\n",
       "          [350.4260, 281.4184, 366.2591, 290.6584],\n",
       "          [ 27.0276, 281.7050,  41.5547, 292.6477],\n",
       "          [ 73.2685, 282.1776,  90.2671, 292.6585],\n",
       "          [371.3147, 257.7279, 388.7963, 282.6366],\n",
       "          [385.2402, 260.8331, 399.8808, 283.9278],\n",
       "          [167.5547, 273.9623, 185.8036, 295.4559],\n",
       "          [358.1330, 274.5198, 378.6673, 283.2240],\n",
       "          [346.2305, 285.6734, 367.2788, 294.4981],\n",
       "          [168.0406, 274.1541, 189.1556, 284.2120],\n",
       "          [265.7310, 246.9177, 394.8713, 295.6706],\n",
       "          [153.1065, 269.7151, 186.5842, 300.0000],\n",
       "          [155.5746, 273.3484, 174.0862, 295.8109],\n",
       "          [155.6791, 274.4554, 178.1502, 284.1723],\n",
       "          [375.3636, 281.7376, 390.1522, 291.4745],\n",
       "          [383.5647, 235.0551, 399.8964, 272.5965],\n",
       "          [377.8811, 294.5738, 390.8339, 300.0000],\n",
       "          [ 85.0487, 281.9092, 102.5365, 292.8057],\n",
       "          [359.2896, 281.3192, 388.8674, 300.0000],\n",
       "          [176.6794, 268.6442, 213.6272, 300.0000],\n",
       "          [  0.0000, 109.1123,  12.1438, 173.8570],\n",
       "          [165.4304, 269.2231, 200.9843, 300.0000],\n",
       "          [ 71.7001, 273.5212,  93.6378, 283.4072],\n",
       "          [370.4384, 223.4095, 399.6547, 289.1812],\n",
       "          [139.6996, 271.7984, 172.7866, 300.0000],\n",
       "          [159.4846, 269.0865, 172.5205, 280.0943],\n",
       "          [385.2547, 261.5988, 399.8752, 269.4498],\n",
       "          [ 85.1900, 273.5905, 106.5709, 283.8039],\n",
       "          [ 67.2695, 273.7864, 104.3890, 300.0000],\n",
       "          [ 17.4879, 294.9125,  32.0130, 300.0000],\n",
       "          [393.8870, 284.7573, 399.9850, 300.0000],\n",
       "          [171.7537, 270.0457, 184.7697, 281.3069],\n",
       "          [209.5826, 269.4860, 246.5695, 299.9915],\n",
       "          [383.0741, 282.8415, 399.9440, 300.0000],\n",
       "          [179.1829, 274.2444, 197.4827, 296.3978],\n",
       "          [203.9769, 273.6445, 223.9748, 282.8570],\n",
       "          [191.7392, 273.8193, 210.3746, 297.2431],\n",
       "          [171.3308, 282.2271, 186.9046, 293.9589],\n",
       "          [ 45.9026, 265.8444,  84.4985, 300.0000],\n",
       "          [158.8361, 281.9137, 175.1746, 293.2980],\n",
       "          [143.3521, 273.1013, 162.8513, 295.5113],\n",
       "          [ 97.2306, 282.3228, 114.8699, 293.2507],\n",
       "          [ 61.5768, 281.7631,  77.3070, 292.3689],\n",
       "          [144.2408, 274.1835, 166.6733, 283.6566],\n",
       "          [  6.8392, 103.6443,  17.1743, 113.9970],\n",
       "          [ 97.1968, 273.9687, 118.2890, 283.8564],\n",
       "          [259.8464, 268.8853, 297.0685, 300.0000],\n",
       "          [ 22.7557, 272.6207,  43.0423, 282.6921],\n",
       "          [346.5352, 274.5593, 368.0226, 283.7258],\n",
       "          [338.4034, 280.9449, 355.2005, 291.4442],\n",
       "          [ 15.3967, 281.9076,  30.0016, 292.3133],\n",
       "          [ 38.8481, 281.9951,  53.9139, 293.2805],\n",
       "          [ 26.2949, 263.8865,  61.3292, 299.9470],\n",
       "          [111.9767,   0.0000, 141.0339,  40.4978],\n",
       "          [ 37.3409, 270.8530,  72.4334, 300.0000],\n",
       "          [ 86.6924, 264.7960, 122.0092, 300.0000],\n",
       "          [  5.9467, 149.5654,  15.4508, 159.9443],\n",
       "          [ 72.7474, 269.1241,  87.2306, 279.1619],\n",
       "          [354.1439, 295.2685, 369.8270, 300.0000],\n",
       "          [326.2846, 269.0502, 340.7803, 279.5362],\n",
       "          [ 84.4208, 285.7407, 104.5727, 295.4567],\n",
       "          [290.3694, 269.1068, 304.2352, 279.6014],\n",
       "          [206.3153, 281.6701, 222.1758, 292.4780],\n",
       "          [  1.4130, 115.0209,  28.1792, 172.8412],\n",
       "          [389.6541, 293.2630, 399.9662, 300.0000],\n",
       "          [222.7476, 270.1791, 260.4049, 300.0000],\n",
       "          [ 13.6233, 285.9991,  44.6987, 300.0000],\n",
       "          [384.9929, 248.9551, 399.9196, 256.7169],\n",
       "          [326.2390, 281.3007, 342.2554, 292.6520],\n",
       "          [ 59.5323, 273.3040,  80.6298, 283.9142],\n",
       "          [ 50.3029, 281.8006,  66.2712, 292.6596],\n",
       "          [ 84.9962, 268.8277,  99.4201, 279.4468],\n",
       "          [318.7794, 271.1440, 354.1413, 299.9883],\n",
       "          [274.5971, 274.1638, 296.5118, 283.6305],\n",
       "          [156.1906, 262.1656, 313.1872, 300.0000],\n",
       "          [126.5036, 272.4842, 159.7950, 300.0000],\n",
       "          [358.3671, 255.6168, 378.2465, 280.5765],\n",
       "          [336.0984, 286.1074, 356.1407, 295.1085],\n",
       "          [302.7223, 268.2689, 316.5666, 278.5828],\n",
       "          [ 60.2576, 285.3819,  79.4487, 295.1649],\n",
       "          [180.0576, 274.3683, 201.0210, 284.2385],\n",
       "          [216.2673, 273.7856, 237.3569, 282.9261],\n",
       "          [305.2338, 295.3794, 319.7305, 300.0000],\n",
       "          [278.2672, 269.6142, 292.0106, 280.4294],\n",
       "          [100.4333, 263.8932, 134.0968, 300.0000],\n",
       "          [183.5309, 281.9686, 198.3083, 293.9790],\n",
       "          [287.2740, 274.4553, 309.0247, 283.9156]], grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1]),\n",
       "  'scores': tensor([0.5182, 0.5178, 0.5170, 0.5162, 0.5161, 0.5157, 0.5155, 0.5153, 0.5151,\n",
       "          0.5150, 0.5150, 0.5150, 0.5147, 0.5145, 0.5144, 0.5141, 0.5141, 0.5141,\n",
       "          0.5139, 0.5135, 0.5135, 0.5134, 0.5133, 0.5132, 0.5132, 0.5131, 0.5128,\n",
       "          0.5128, 0.5126, 0.5125, 0.5125, 0.5122, 0.5122, 0.5121, 0.5120, 0.5118,\n",
       "          0.5118, 0.5115, 0.5114, 0.5111, 0.5111, 0.5108, 0.5107, 0.5105, 0.5104,\n",
       "          0.5101, 0.5100, 0.5099, 0.5099, 0.5098, 0.5098, 0.5097, 0.5097, 0.5096,\n",
       "          0.5096, 0.5095, 0.5094, 0.5092, 0.5092, 0.5090, 0.5090, 0.5090, 0.5090,\n",
       "          0.5089, 0.5089, 0.5089, 0.5087, 0.5087, 0.5087, 0.5086, 0.5085, 0.5083,\n",
       "          0.5083, 0.5082, 0.5081, 0.5081, 0.5080, 0.5080, 0.5079, 0.5077, 0.5077,\n",
       "          0.5077, 0.5077, 0.5077, 0.5077, 0.5077, 0.5076, 0.5075, 0.5075, 0.5074,\n",
       "          0.5074, 0.5074, 0.5073, 0.5072, 0.5071, 0.5069, 0.5068, 0.5068, 0.5068,\n",
       "          0.5068], grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([[3.4649e+02, 3.3135e+02, 3.9943e+02, 5.0000e+02],\n",
       "          [3.2570e+02, 3.0944e-01, 3.9996e+02, 3.7961e+01],\n",
       "          [3.3781e+02, 1.6960e-01, 3.7948e+02, 1.4392e+01],\n",
       "          [2.2146e+02, 4.4300e+02, 3.9901e+02, 5.0000e+02],\n",
       "          [3.3898e+02, 4.5294e+02, 3.7415e+02, 5.0000e+02],\n",
       "          [3.0959e+02, 4.7017e+02, 3.2778e+02, 4.8391e+02],\n",
       "          [2.7891e+02, 4.1643e+02, 3.9943e+02, 4.9032e+02],\n",
       "          [3.4605e+02, 4.0767e+02, 3.9954e+02, 4.6534e+02],\n",
       "          [3.2136e+02, 4.6735e+02, 3.6806e+02, 5.0000e+02],\n",
       "          [1.2763e+02, 4.6014e+02, 1.5593e+02, 4.7195e+02],\n",
       "          [2.9353e+02, 4.6960e+02, 3.1191e+02, 4.8343e+02],\n",
       "          [3.4845e+02, 1.6203e+02, 3.8829e+02, 2.0817e+02],\n",
       "          [1.8048e+02, 4.5541e+02, 1.9614e+02, 4.6836e+02],\n",
       "          [3.3486e+02, 3.8665e+02, 3.9935e+02, 4.4497e+02],\n",
       "          [1.6604e+02, 4.7087e+02, 1.8475e+02, 4.8458e+02],\n",
       "          [1.8239e+02, 4.7136e+02, 2.0105e+02, 4.8495e+02],\n",
       "          [1.9645e+02, 4.5528e+02, 2.1209e+02, 4.6792e+02],\n",
       "          [1.3308e+02, 4.7109e+02, 1.5219e+02, 4.8440e+02],\n",
       "          [1.9837e+02, 4.7142e+02, 2.1656e+02, 4.8462e+02],\n",
       "          [3.3209e+02, 1.6376e+02, 3.9945e+02, 2.2204e+02],\n",
       "          [3.7558e+01, 4.7021e+02, 5.5436e+01, 4.8425e+02],\n",
       "          [2.2162e+01, 4.6991e+02, 4.0208e+01, 4.8391e+02],\n",
       "          [1.1659e+02, 4.5606e+02, 1.3261e+02, 4.6943e+02],\n",
       "          [1.6404e+02, 4.5466e+02, 1.7973e+02, 4.6797e+02],\n",
       "          [3.4507e+02, 4.6075e+02, 3.9944e+02, 5.0000e+02],\n",
       "          [3.6929e+02, 3.8429e+02, 3.9961e+02, 4.6970e+02],\n",
       "          [1.0075e+02, 4.5624e+02, 1.1651e+02, 4.7014e+02],\n",
       "          [3.8302e+02, 3.9295e+02, 3.9985e+02, 4.8531e+02],\n",
       "          [1.4970e+02, 4.7106e+02, 1.6825e+02, 4.8476e+02],\n",
       "          [9.5171e+01, 4.6083e+02, 1.2397e+02, 4.7226e+02],\n",
       "          [1.1728e+02, 4.7160e+02, 1.3652e+02, 4.8536e+02],\n",
       "          [1.8126e+02, 4.4102e+02, 1.9564e+02, 4.5451e+02],\n",
       "          [3.4934e+02, 4.3568e+02, 3.8407e+02, 5.0000e+02],\n",
       "          [2.1180e+02, 4.5538e+02, 2.2723e+02, 4.6803e+02],\n",
       "          [1.6011e+01, 4.5931e+02, 4.4851e+01, 4.7133e+02],\n",
       "          [2.1386e+02, 5.2982e+01, 2.5239e+02, 1.4262e+02],\n",
       "          [1.3237e+02, 4.5497e+02, 1.4803e+02, 4.6845e+02],\n",
       "          [3.4415e+02, 1.7877e+02, 3.8638e+02, 2.2132e+02],\n",
       "          [1.0146e+02, 4.7195e+02, 1.2059e+02, 4.8588e+02],\n",
       "          [3.2872e+02, 2.7169e+02, 3.9951e+02, 4.3916e+02],\n",
       "          [3.4808e+02, 4.4520e+02, 3.7779e+02, 4.5589e+02],\n",
       "          [3.4470e+02, 1.4312e+01, 3.8374e+02, 6.1361e+01],\n",
       "          [3.1990e+02, 1.3226e-01, 3.6122e+02, 3.3610e+01],\n",
       "          [2.3841e+02, 3.5835e+02, 3.9923e+02, 5.0000e+02],\n",
       "          [3.2938e+02, 1.7384e+01, 3.9939e+02, 7.5962e+01],\n",
       "          [2.2395e+01, 4.5618e+02, 3.7464e+01, 4.7080e+02],\n",
       "          [2.6175e+02, 4.7095e+02, 2.7944e+02, 4.8434e+02],\n",
       "          [3.4655e+02, 3.7579e+02, 3.9946e+02, 4.2427e+02],\n",
       "          [2.7815e+02, 4.6976e+02, 2.9572e+02, 4.8350e+02],\n",
       "          [2.6207e+02, 4.5594e+02, 2.7715e+02, 4.6900e+02],\n",
       "          [1.6512e+02, 4.4031e+02, 1.7921e+02, 4.5384e+02],\n",
       "          [1.9801e+02, 2.9534e+01, 2.3656e+02, 1.2252e+02],\n",
       "          [2.3031e+02, 5.3158e+01, 2.6903e+02, 1.4398e+02],\n",
       "          [2.1408e+02, 4.7143e+02, 2.3160e+02, 4.8448e+02],\n",
       "          [3.3914e+02, 1.6078e+02, 3.7637e+02, 2.3045e+02],\n",
       "          [3.7662e+02, 4.1170e+02, 3.9977e+02, 4.5874e+02],\n",
       "          [8.4114e+01, 4.5570e+02, 1.0018e+02, 4.6968e+02],\n",
       "          [3.3039e+02, 3.5628e+02, 3.9944e+02, 4.0965e+02],\n",
       "          [3.7951e+01, 4.5575e+02, 5.3426e+01, 4.6947e+02],\n",
       "          [8.4769e+00, 1.0449e+02, 2.0842e+01, 1.1799e+02],\n",
       "          [3.7972e+02, 4.6210e+02, 3.9983e+02, 4.8621e+02],\n",
       "          [2.0967e+01, 2.9032e+02, 6.0413e+01, 3.7651e+02],\n",
       "          [2.4469e+02, 4.5616e+02, 2.5991e+02, 4.6910e+02],\n",
       "          [5.2651e+01, 4.7168e+02, 7.1474e+01, 4.8550e+02],\n",
       "          [6.8254e+01, 4.5583e+02, 8.3918e+01, 4.6928e+02],\n",
       "          [1.4791e+02, 4.5457e+02, 1.6321e+02, 4.6800e+02],\n",
       "          [2.1042e+01, 2.0945e+02, 6.0185e+01, 2.9736e+02],\n",
       "          [3.4515e+02, 1.9580e+02, 3.8949e+02, 2.3887e+02],\n",
       "          [2.2995e+02, 4.7184e+02, 2.4684e+02, 4.8496e+02],\n",
       "          [3.3178e+02, 2.1290e+02, 3.9939e+02, 2.6462e+02],\n",
       "          [3.1942e+02, 4.3492e-02, 3.6249e+02, 1.0653e+01],\n",
       "          [3.8980e+02, 4.6749e+02, 3.9996e+02, 4.8115e+02],\n",
       "          [6.1048e+00, 4.7038e+02, 2.5035e+01, 4.8450e+02],\n",
       "          [3.3060e+02, 2.9071e+02, 3.9959e+02, 3.3872e+02],\n",
       "          [8.3535e+00, 2.3216e+02, 2.2138e+01, 2.4623e+02],\n",
       "          [3.4859e+02, 2.1713e-01, 3.9993e+02, 1.7793e+01],\n",
       "          [1.6757e+02, 4.2469e+02, 2.0802e+02, 4.9983e+02],\n",
       "          [2.2759e+02, 4.5615e+02, 2.4318e+02, 4.6892e+02],\n",
       "          [2.4615e+02, 5.6855e+01, 2.8430e+02, 1.4371e+02],\n",
       "          [2.4640e+02, 4.7129e+02, 2.6337e+02, 4.8480e+02],\n",
       "          [9.3838e+00, 8.9308e+01, 2.1184e+01, 1.0242e+02],\n",
       "          [1.5183e+02, 4.3083e+02, 1.9534e+02, 4.9513e+02],\n",
       "          [1.9859e+02, 6.8137e+01, 2.3849e+02, 1.5816e+02],\n",
       "          [3.4947e+02, 2.4756e+02, 3.9268e+02, 2.9328e+02],\n",
       "          [1.1836e+02, 2.2646e+01, 1.5865e+02, 1.1284e+02],\n",
       "          [8.4558e+01, 4.7207e+02, 1.0396e+02, 4.8617e+02],\n",
       "          [6.8436e+01, 4.7193e+02, 8.7606e+01, 4.8607e+02],\n",
       "          [3.4725e+02, 2.3285e+02, 3.9219e+02, 2.7743e+02],\n",
       "          [3.2941e+02, 1.3046e+02, 3.9948e+02, 1.8836e+02],\n",
       "          [3.4066e+02, 2.6797e+02, 3.9952e+02, 3.1634e+02],\n",
       "          [3.2082e+02, 1.3129e+02, 3.9918e+02, 3.1014e+02],\n",
       "          [1.8841e+01, 2.4343e+02, 5.8367e+01, 3.3059e+02],\n",
       "          [3.2267e+02, 6.9026e+00, 3.5968e+02, 5.4943e+01],\n",
       "          [2.4737e+01, 3.4591e+02, 3.6920e+01, 3.5807e+02],\n",
       "          [1.9950e+02, 2.6330e+02, 2.3826e+02, 3.4822e+02],\n",
       "          [0.0000e+00, 6.1804e+01, 3.4931e+01, 1.4678e+02],\n",
       "          [0.0000e+00, 2.2907e+02, 1.7098e+01, 2.8693e+02],\n",
       "          [7.6558e+00, 2.9472e+02, 2.1081e+01, 3.0838e+02],\n",
       "          [3.2042e+02, 3.0136e+02, 3.9952e+02, 3.6072e+02],\n",
       "          [2.1443e+02, 2.4685e+02, 2.5306e+02, 3.3324e+02]],\n",
       "         grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1]),\n",
       "  'scores': tensor([0.5243, 0.5225, 0.5218, 0.5217, 0.5211, 0.5196, 0.5193, 0.5173, 0.5164,\n",
       "          0.5161, 0.5156, 0.5153, 0.5151, 0.5142, 0.5142, 0.5139, 0.5137, 0.5135,\n",
       "          0.5134, 0.5132, 0.5132, 0.5131, 0.5131, 0.5130, 0.5130, 0.5123, 0.5122,\n",
       "          0.5122, 0.5121, 0.5118, 0.5118, 0.5117, 0.5116, 0.5116, 0.5114, 0.5114,\n",
       "          0.5114, 0.5112, 0.5111, 0.5111, 0.5111, 0.5110, 0.5108, 0.5107, 0.5104,\n",
       "          0.5104, 0.5104, 0.5102, 0.5102, 0.5101, 0.5100, 0.5100, 0.5099, 0.5099,\n",
       "          0.5097, 0.5096, 0.5095, 0.5094, 0.5093, 0.5092, 0.5090, 0.5089, 0.5088,\n",
       "          0.5088, 0.5085, 0.5084, 0.5084, 0.5083, 0.5083, 0.5082, 0.5082, 0.5081,\n",
       "          0.5081, 0.5080, 0.5079, 0.5079, 0.5078, 0.5077, 0.5077, 0.5074, 0.5073,\n",
       "          0.5073, 0.5072, 0.5072, 0.5071, 0.5070, 0.5068, 0.5064, 0.5064, 0.5063,\n",
       "          0.5062, 0.5062, 0.5061, 0.5061, 0.5061, 0.5059, 0.5059, 0.5059, 0.5058,\n",
       "          0.5058], grad_fn=<IndexBackward0>)}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, we're going to build our own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import warnings\n",
    "from typing import Tuple, List, Dict, Optional, Union\n",
    "\n",
    "\n",
    "class MultimodalGeneralizedRCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Main class for Generalized R-CNN.\n",
    "\n",
    "    Args:\n",
    "        backbone (nn.Module):\n",
    "        rpn (nn.Module):\n",
    "        roi_heads (nn.Module): takes the features + the proposals from the RPN and computes\n",
    "            detections / masks from it.\n",
    "        transform (nn.Module): performs the data transformation from the inputs to feed into\n",
    "            the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone, rpn, roi_heads, transform):\n",
    "        super(MultimodalGeneralizedRCNN, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.backbone = backbone\n",
    "        self.rpn = rpn\n",
    "        self.roi_heads = roi_heads\n",
    "        # used only on torchscript mode\n",
    "        self._has_warned = False\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def eager_outputs(self, losses, detections):\n",
    "        # type: (Dict[str, Tensor], List[Dict[str, Tensor]]) -> Union[Dict[str, Tensor], List[Dict[str, Tensor]]]\n",
    "        if self.training:\n",
    "            return losses\n",
    "\n",
    "        return detections\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (list[Tensor]): images to be processed\n",
    "            targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional)\n",
    "\n",
    "        Returns:\n",
    "            result (list[BoxList] or dict[Tensor]): the output from the model.\n",
    "                During training, it returns a dict[Tensor] which contains the losses.\n",
    "                During testing, it returns list[BoxList] contains additional fields\n",
    "                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
    "\n",
    "        \"\"\"\n",
    "        if self.training and targets is None:\n",
    "            raise ValueError(\"In training mode, targets should be passed\")\n",
    "        if self.training:\n",
    "            assert targets is not None\n",
    "            for target in targets:\n",
    "                boxes = target[\"boxes\"]\n",
    "                if isinstance(boxes, torch.Tensor):\n",
    "                    if len(boxes.shape) != 2 or boxes.shape[-1] != 4:\n",
    "                        raise ValueError(\"Expected target boxes to be a tensor\"\n",
    "                                         \"of shape [N, 4], got {:}.\".format(\n",
    "                                             boxes.shape))\n",
    "                else:\n",
    "                    raise ValueError(\"Expected target boxes to be of type \"\n",
    "                                     \"Tensor, got {:}.\".format(type(boxes)))\n",
    "\n",
    "        original_image_sizes: List[Tuple[int, int]] = []\n",
    "        for img in images:\n",
    "            val = img.shape[-2:]\n",
    "            assert len(val) == 2\n",
    "            original_image_sizes.append((val[0], val[1]))\n",
    "\n",
    "        images, targets = self.transform(images, targets)\n",
    "\n",
    "        self.images_after_trans = images\n",
    "\n",
    "        # Check for degenerate boxes\n",
    "        # TODO: Move this to a function\n",
    "        if targets is not None:\n",
    "            for target_idx, target in enumerate(targets):\n",
    "                boxes = target[\"boxes\"]\n",
    "                degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n",
    "                if degenerate_boxes.any():\n",
    "                    # print the first degenerate box\n",
    "                    bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n",
    "                    degen_bb: List[float] = boxes[bb_idx].tolist()\n",
    "                    raise ValueError(\"All bounding boxes should have positive height and width.\"\n",
    "                                     \" Found invalid box {} for target at index {}.\"\n",
    "                                     .format(degen_bb, target_idx))\n",
    "\n",
    "        features = self.backbone(images.tensors)\n",
    "        self.features = features\n",
    "\n",
    "        if isinstance(features, torch.Tensor):\n",
    "            features = OrderedDict([('0', features)])\n",
    "        proposals, proposal_losses = self.rpn(images, features, targets)\n",
    "        detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)\n",
    "        detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)\n",
    "\n",
    "        self.detections = detections\n",
    "\n",
    "        losses = {}\n",
    "        losses.update(detector_losses)\n",
    "        losses.update(proposal_losses)\n",
    "\n",
    "        if torch.jit.is_scripting():\n",
    "            if not self._has_warned:\n",
    "                warnings.warn(\"RCNN always returns a (Losses, Detections) tuple in scripting\")\n",
    "                self._has_warned = True\n",
    "            return losses, detections\n",
    "        else:\n",
    "            return self.eager_outputs(losses, detections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "from torchvision.models.detection.faster_rcnn import MultiScaleRoIAlign, RPNHead, RegionProposalNetwork, TwoMLPHead, FastRCNNPredictor, RoIHeads\n",
    "\n",
    "class MultimodalFasterRCNN(MultimodalGeneralizedRCNN):\n",
    "    \"\"\"\n",
    "    Implements Faster R-CNN.\n",
    "\n",
    "    The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each\n",
    "    image, and should be in 0-1 range. Different images can have different sizes.\n",
    "\n",
    "    The behavior of the model changes depending if it is in training or evaluation mode.\n",
    "\n",
    "    During training, the model expects both the input tensors, as well as a targets (list of dictionary),\n",
    "    containing:\n",
    "        - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with\n",
    "          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
    "        - labels (Int64Tensor[N]): the class label for each ground-truth box\n",
    "\n",
    "    The model returns a Dict[Tensor] during training, containing the classification and regression\n",
    "    losses for both the RPN and the R-CNN.\n",
    "\n",
    "    During inference, the model requires only the input tensors, and returns the post-processed\n",
    "    predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as\n",
    "    follows:\n",
    "        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with\n",
    "          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
    "        - labels (Int64Tensor[N]): the predicted labels for each image\n",
    "        - scores (Tensor[N]): the scores or each prediction\n",
    "\n",
    "    Args:\n",
    "        backbone (nn.Module): the network used to compute the features for the model.\n",
    "            It should contain a out_channels attribute, which indicates the number of output\n",
    "            channels that each feature map has (and it should be the same for all feature maps).\n",
    "            The backbone should return a single Tensor or and OrderedDict[Tensor].\n",
    "        num_classes (int): number of output classes of the model (including the background).\n",
    "            If box_predictor is specified, num_classes should be None.\n",
    "        min_size (int): minimum size of the image to be rescaled before feeding it to the backbone\n",
    "        max_size (int): maximum size of the image to be rescaled before feeding it to the backbone\n",
    "        image_mean (Tuple[float, float, float]): mean values used for input normalization.\n",
    "            They are generally the mean values of the dataset on which the backbone has been trained\n",
    "            on\n",
    "        image_std (Tuple[float, float, float]): std values used for input normalization.\n",
    "            They are generally the std values of the dataset on which the backbone has been trained on\n",
    "        rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature\n",
    "            maps.\n",
    "        rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN\n",
    "        rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training\n",
    "        rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing\n",
    "        rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training\n",
    "        rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing\n",
    "        rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals\n",
    "        rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be\n",
    "            considered as positive during training of the RPN.\n",
    "        rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be\n",
    "            considered as negative during training of the RPN.\n",
    "        rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN\n",
    "            for computing the loss\n",
    "        rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training\n",
    "            of the RPN\n",
    "        rpn_score_thresh (float): during inference, only return proposals with a classification score\n",
    "            greater than rpn_score_thresh\n",
    "        box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in\n",
    "            the locations indicated by the bounding boxes\n",
    "        box_head (nn.Module): module that takes the cropped feature maps as input\n",
    "        box_predictor (nn.Module): module that takes the output of box_head and returns the\n",
    "            classification logits and box regression deltas.\n",
    "        box_score_thresh (float): during inference, only return proposals with a classification score\n",
    "            greater than box_score_thresh\n",
    "        box_nms_thresh (float): NMS threshold for the prediction head. Used during inference\n",
    "        box_detections_per_img (int): maximum number of detections per image, for all classes.\n",
    "        box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be\n",
    "            considered as positive during training of the classification head\n",
    "        box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be\n",
    "            considered as negative during training of the classification head\n",
    "        box_batch_size_per_image (int): number of proposals that are sampled during training of the\n",
    "            classification head\n",
    "        box_positive_fraction (float): proportion of positive proposals in a mini-batch during training\n",
    "            of the classification head\n",
    "        bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the\n",
    "            bounding boxes\n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> import torch\n",
    "        >>> import torchvision\n",
    "        >>> from torchvision.models.detection import FasterRCNN\n",
    "        >>> from torchvision.models.detection.rpn import AnchorGenerator\n",
    "        >>> # load a pre-trained model for classification and return\n",
    "        >>> # only the features\n",
    "        >>> backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "        >>> # FasterRCNN needs to know the number of\n",
    "        >>> # output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "        >>> # so we need to add it here\n",
    "        >>> backbone.out_channels = 1280\n",
    "        >>>\n",
    "        >>> # let's make the RPN generate 5 x 3 anchors per spatial\n",
    "        >>> # location, with 5 different sizes and 3 different aspect\n",
    "        >>> # ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "        >>> # map could potentially have different sizes and\n",
    "        >>> # aspect ratios\n",
    "        >>> anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "        >>>                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "        >>>\n",
    "        >>> # let's define what are the feature maps that we will\n",
    "        >>> # use to perform the region of interest cropping, as well as\n",
    "        >>> # the size of the crop after rescaling.\n",
    "        >>> # if your backbone returns a Tensor, featmap_names is expected to\n",
    "        >>> # be ['0']. More generally, the backbone should return an\n",
    "        >>> # OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "        >>> # feature maps to use.\n",
    "        >>> roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "        >>>                                                 output_size=7,\n",
    "        >>>                                                 sampling_ratio=2)\n",
    "        >>>\n",
    "        >>> # put the pieces together inside a FasterRCNN model\n",
    "        >>> model = FasterRCNN(backbone,\n",
    "        >>>                    num_classes=2,\n",
    "        >>>                    rpn_anchor_generator=anchor_generator,\n",
    "        >>>                    box_roi_pool=roi_pooler)\n",
    "        >>> model.eval()\n",
    "        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "        >>> predictions = model(x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone, num_classes=None,\n",
    "                 # transform parameters\n",
    "                 min_size=800, max_size=1333,\n",
    "                 image_mean=None, image_std=None,\n",
    "                 # RPN parameters\n",
    "                 rpn_anchor_generator=None, rpn_head=None,\n",
    "                 rpn_pre_nms_top_n_train=2000, rpn_pre_nms_top_n_test=1000,\n",
    "                 rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=1000,\n",
    "                 rpn_nms_thresh=0.7,\n",
    "                 rpn_fg_iou_thresh=0.7, rpn_bg_iou_thresh=0.3,\n",
    "                 rpn_batch_size_per_image=256, rpn_positive_fraction=0.5,\n",
    "                 rpn_score_thresh=0.0,\n",
    "                 # Box parameters\n",
    "                 box_roi_pool=None, box_head=None, box_predictor=None,\n",
    "                 box_score_thresh=0.05, box_nms_thresh=0.5, box_detections_per_img=100,\n",
    "                 box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5,\n",
    "                 box_batch_size_per_image=512, box_positive_fraction=0.25,\n",
    "                 bbox_reg_weights=None):\n",
    "\n",
    "        if not hasattr(backbone, \"out_channels\"):\n",
    "            raise ValueError(\n",
    "                \"backbone should contain an attribute out_channels \"\n",
    "                \"specifying the number of output channels (assumed to be the \"\n",
    "                \"same for all the levels)\")\n",
    "\n",
    "        assert isinstance(rpn_anchor_generator, (AnchorGenerator, type(None)))\n",
    "        assert isinstance(box_roi_pool, (MultiScaleRoIAlign, type(None)))\n",
    "\n",
    "        if num_classes is not None:\n",
    "            if box_predictor is not None:\n",
    "                raise ValueError(\"num_classes should be None when box_predictor is specified\")\n",
    "        else:\n",
    "            if box_predictor is None:\n",
    "                raise ValueError(\"num_classes should not be None when box_predictor \"\n",
    "                                 \"is not specified\")\n",
    "\n",
    "        out_channels = backbone.out_channels\n",
    "\n",
    "        if rpn_anchor_generator is None:\n",
    "            anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n",
    "            aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "            rpn_anchor_generator = AnchorGenerator(\n",
    "                anchor_sizes, aspect_ratios\n",
    "            )\n",
    "        if rpn_head is None:\n",
    "            rpn_head = RPNHead(\n",
    "                out_channels, rpn_anchor_generator.num_anchors_per_location()[0]\n",
    "            )\n",
    "\n",
    "        rpn_pre_nms_top_n = dict(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)\n",
    "        rpn_post_nms_top_n = dict(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)\n",
    "\n",
    "        rpn = RegionProposalNetwork(\n",
    "            rpn_anchor_generator, rpn_head,\n",
    "            rpn_fg_iou_thresh, rpn_bg_iou_thresh,\n",
    "            rpn_batch_size_per_image, rpn_positive_fraction,\n",
    "            rpn_pre_nms_top_n, rpn_post_nms_top_n, rpn_nms_thresh,\n",
    "            score_thresh=rpn_score_thresh)\n",
    "\n",
    "        if box_roi_pool is None:\n",
    "            box_roi_pool = MultiScaleRoIAlign(\n",
    "                featmap_names=['0', '1', '2', '3'],\n",
    "                output_size=7,\n",
    "                sampling_ratio=2)\n",
    "\n",
    "        if box_head is None:\n",
    "            resolution = box_roi_pool.output_size[0]\n",
    "            representation_size = 1024\n",
    "            box_head = TwoMLPHead(\n",
    "                out_channels * resolution ** 2,\n",
    "                representation_size)\n",
    "\n",
    "        if box_predictor is None:\n",
    "            representation_size = 1024\n",
    "            box_predictor = FastRCNNPredictor(\n",
    "                representation_size,\n",
    "                num_classes)\n",
    "\n",
    "        roi_heads = RoIHeads(\n",
    "            # Box\n",
    "            box_roi_pool, box_head, box_predictor,\n",
    "            box_fg_iou_thresh, box_bg_iou_thresh,\n",
    "            box_batch_size_per_image, box_positive_fraction,\n",
    "            bbox_reg_weights,\n",
    "            box_score_thresh, box_nms_thresh, box_detections_per_img)\n",
    "\n",
    "        if image_mean is None:\n",
    "            image_mean = [0.485, 0.456, 0.406]\n",
    "        if image_std is None:\n",
    "            image_std = [0.229, 0.224, 0.225]\n",
    "        transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std)\n",
    "\n",
    "        super(MultimodalFasterRCNN, self).__init__(backbone, rpn, roi_heads, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "clinical_feature_size = 35\n",
    "example_clinical_data = torch.rand(batch_size, clinical_feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 35])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_clinical_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "deconv = torch.nn.ConvTranspose2d(35, 20, (25, 36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "630020"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([param.nelement() for param in deconv.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 60, 25, 36])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deconv(example_clinical_data[:, :, None, None]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = MultimodalFasterRCNN(backbone,\n",
    "                    # max_size=800,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultimodalFasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): Sequential(\n",
       "    (0): ConvNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): ConvNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(1280, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(1280, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=62720, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "output = model(images, targets)  # Returns losses and detections\n",
    "# For inference\n",
    "# model.eval()\n",
    "# x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "# predictions = model(x)  # Returns predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'boxes': tensor([[438., 246., 536., 521.],\n",
       "          [199., 258., 269., 528.],\n",
       "          [251., 255., 373., 527.]]),\n",
       "  'labels': tensor([1, 1, 1]),\n",
       "  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       "  'image_id': tensor([55]),\n",
       "  'area': tensor([26950., 18900., 33184.]),\n",
       "  'iscrowd': tensor([0, 0, 0])},\n",
       " {'boxes': tensor([[ 21.,  75., 167., 366.],\n",
       "          [223.,  92., 257., 166.],\n",
       "          [422.,  94., 444., 159.]]),\n",
       "  'labels': tensor([1, 1, 1]),\n",
       "  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       "  'image_id': tensor([20]),\n",
       "  'area': tensor([42486.,  2516.,  1430.]),\n",
       "  'iscrowd': tensor([0, 0, 0])})"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1280, 25, 36])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a402e4e4296f2d4bed1c089fb7c7e828933dcbfe50698b381e393c052eea855"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
