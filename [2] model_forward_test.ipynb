{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out our model here.\n",
    "\n",
    "We test our mutli-modal Faster R-CNN with MIMIC dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from data.dataset import REFLACXWithClinicalAndBoundingBoxDataset\n",
    "from utils.transforms import get_transform\n",
    "\n",
    "## Suppress the assignement warning from pandas.\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your MIMIC folde path here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "XAMI_MIMIC_PATH = \"D:\\XAMI-MIMIC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate datasets and dataloaders\n",
    "The batch size is also defined in this section. For testing purpose, we only set it as 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = REFLACXWithClinicalAndBoundingBoxDataset(\n",
    "    XAMI_MIMIC_PATH=XAMI_MIMIC_PATH,\n",
    "    split_str=\"train\",\n",
    "    transforms=get_transform(train=True),\n",
    ")\n",
    "\n",
    "val_dataset = REFLACXWithClinicalAndBoundingBoxDataset(\n",
    "    XAMI_MIMIC_PATH=XAMI_MIMIC_PATH,\n",
    "    split_str=\"val\",\n",
    "    transforms=get_transform(train=False),\n",
    ")\n",
    "\n",
    "test_dataset = REFLACXWithClinicalAndBoundingBoxDataset(\n",
    "    XAMI_MIMIC_PATH=XAMI_MIMIC_PATH,\n",
    "    split_str=\"test\",\n",
    "    transforms=get_transform(train=False),\n",
    ")\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=REFLACXWithClinicalAndBoundingBoxDataset.collate_fn\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=True, collate_fn=REFLACXWithClinicalAndBoundingBoxDataset.collate_fn\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=True, collate_fn=REFLACXWithClinicalAndBoundingBoxDataset.collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example instance from dataset\n",
    "We show what's inside a single instance. It will provide:\n",
    "\n",
    "- Images\n",
    "- Clinical data\n",
    "- Targets (Dictionary)\n",
    "\n",
    "And, inside the target, there're:\n",
    "\n",
    "- boxes (bounding boxes of abnormality)\n",
    "- lable (disease index (Note: the class **0** means the background))\n",
    "- image_id (idx to get that image)\n",
    "- area (the areas that bouding boxes contain)\n",
    "- iscrowd (if it's a place with multiple bouding boxes, we assume all the the bouding boxes are not crowd.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.5608, 0.5608, 0.5569,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5569, 0.5647, 0.5647,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5490, 0.5569, 0.5608,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.7686, 0.7686, 0.7647,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.7608, 0.7647, 0.7608,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.7529, 0.7569, 0.7569,  ..., 1.0000, 1.0000, 1.0000]],\n",
       " \n",
       "         [[0.5608, 0.5608, 0.5569,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5569, 0.5647, 0.5647,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5490, 0.5569, 0.5608,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.7686, 0.7686, 0.7647,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.7608, 0.7647, 0.7608,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.7529, 0.7569, 0.7569,  ..., 1.0000, 1.0000, 1.0000]],\n",
       " \n",
       "         [[0.5608, 0.5608, 0.5569,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5569, 0.5647, 0.5647,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5490, 0.5569, 0.5608,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.7686, 0.7686, 0.7647,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.7608, 0.7647, 0.7608,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.7529, 0.7569, 0.7569,  ..., 1.0000, 1.0000, 1.0000]]]),\n",
       " tensor([ 69.0000,  98.1000,  90.0000,  18.0000,  99.0000, 184.0000,  75.0000,\n",
       "          13.0000,   3.0000]),\n",
       " tensor([0], dtype=torch.int32),\n",
       " {'boxes': tensor([[ 734., 1204., 2211., 2175.]], dtype=torch.float64),\n",
       "  'labels': tensor([1]),\n",
       "  'image_id': tensor([0]),\n",
       "  'area': tensor([1434167.], dtype=torch.float64),\n",
       "  'iscrowd': tensor([0])})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model.\n",
    "\n",
    "We define he models here. Two backbone examples are in the below code section. The MobileNet is a light weight network, and ResNet is heavier, but usually perform better. In our case, the calculation is not the most important factor; therefore, we chose ResNet with feature pyramid networks (FPN) backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from models.rcnn import MultimodalFasterRCNN\n",
    "\n",
    "trainable_backbone_layers = torchvision.models.detection.backbone_utils._validate_trainable_layers(\n",
    "    True, None, 5, 3\n",
    ")\n",
    "backbone = torchvision.models.detection.backbone_utils.resnet_fpn_backbone(\n",
    "    \"resnet50\", pretrained=True, trainable_layers=trainable_backbone_layers\n",
    ")\n",
    "backbone.out_channels = 256\n",
    "\n",
    "\n",
    "######################## For MobileNet backbone ########################\n",
    "# from torchvision.models.detection.faster_rcnn import AnchorGenerator\n",
    "# backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# backbone.out_channels = 1280\n",
    "# anchor_generator = AnchorGenerator(\n",
    "#     sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "# )\n",
    "# roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "#     featmap_names=[\"0\"], output_size=7, sampling_ratio=2\n",
    "# )\n",
    "########################################################################\n",
    "\n",
    "model = MultimodalFasterRCNN(\n",
    "    backbone,\n",
    "    num_classes=len(train_dataset.labels_cols) + 1,\n",
    "    rpn_anchor_generator=None,\n",
    "    box_roi_pool=None,\n",
    "    use_clinical=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data to feed\n",
    "\n",
    "We prepare three main data to test the model:\n",
    "\n",
    "- CXR image\n",
    "- Clinical data\n",
    "- Target\n",
    "\n",
    "And, for each data, we adjust the format to what the model expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, clinical_num, clinical_cat, targets = next(iter(train_dataloader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Feedforawrd (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mike8\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "output = model(images, (clinical_num, clinical_cat), targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results we get.\n",
    "Four different losses are given in the result, we will use these losses to optimise the network while training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_classifier': tensor(1.7564, grad_fn=<NllLossBackward0>),\n",
       " 'loss_box_reg': tensor(0., grad_fn=<DivBackward0>),\n",
       " 'loss_objectness': tensor(0.6937, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " 'loss_rpn_box_reg': tensor(0., grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Feedforawrd (Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred = model(images, (clinical_num, clinical_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results we get.\n",
    "If we set the model to evaluation mode and don't pass the target to the forward function, the model will output prediction (detections). In the below sections, we show what's inside the detection of first instance (idx=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection.\n",
    "\n",
    "A detection contain *boxes*, *lables*, and *scores*.\n",
    "\n",
    "- *boxes*: All the bounding boxes for this image. \n",
    "- *lables*: Labels corresponded to the bounding boxes.\n",
    "- *score*: Score (Confidence) for each boudning box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['boxes', 'labels', 'scores'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7698e+02, 1.2941e+03, 6.2344e+02, 2.3637e+03],\n",
       "        [8.1306e+02, 1.5948e+03, 9.7100e+02, 1.7924e+03],\n",
       "        [3.1414e+02, 1.5101e+03, 9.4729e+02, 2.2719e+03],\n",
       "        [3.2322e+02, 8.4963e+02, 9.4594e+02, 1.6075e+03],\n",
       "        [8.1714e+02, 3.7854e+00, 2.0837e+03, 1.0938e+03],\n",
       "        [1.6042e+03, 1.0259e+03, 2.0531e+03, 2.0976e+03],\n",
       "        [1.5851e+00, 4.8525e+02, 6.1034e+02, 2.6296e+03],\n",
       "        [7.7397e+02, 1.6425e+03, 9.3297e+02, 1.8400e+03],\n",
       "        [8.3674e+02, 1.2631e+03, 1.0656e+03, 1.8032e+03],\n",
       "        [5.5690e+02, 3.7163e+02, 1.1832e+03, 1.1342e+03],\n",
       "        [7.9909e+02, 1.4023e+03, 1.0283e+03, 1.9429e+03],\n",
       "        [7.1944e+02, 1.4034e+03, 9.4915e+02, 1.9444e+03],\n",
       "        [4.7966e+02, 2.9425e+00, 1.1110e+03, 6.6280e+02],\n",
       "        [3.2187e+02, 8.4664e+02, 9.4967e+02, 1.6122e+03],\n",
       "        [7.7631e+02, 1.5262e+03, 9.3498e+02, 1.7207e+03],\n",
       "        [3.4360e+00, 1.3271e+03, 9.6685e+02, 2.8294e+03],\n",
       "        [6.3905e+02, 8.6920e+01, 1.2667e+03, 8.4386e+02],\n",
       "        [5.1388e+00, 3.4922e+00, 1.0978e+03, 9.1821e+02],\n",
       "        [9.6006e+02, 2.0875e+02, 1.1865e+03, 7.4153e+02],\n",
       "        [2.0003e+03, 3.1593e+02, 2.4507e+03, 1.4190e+03],\n",
       "        [4.9762e+02, 1.6891e+03, 6.5602e+02, 1.8892e+03],\n",
       "        [8.0041e+02, 3.5837e+02, 1.0295e+03, 8.8756e+02],\n",
       "        [8.7453e+02, 4.3525e+00, 1.5070e+03, 6.5982e+02],\n",
       "        [2.5486e+02, 1.6682e+03, 7.0135e+02, 2.7344e+03],\n",
       "        [6.3733e+02, 8.1717e+01, 1.2721e+03, 8.4721e+02],\n",
       "        [1.7646e+03, 3.3522e+02, 2.2092e+03, 1.4135e+03],\n",
       "        [7.1277e+02, 1.5951e+03, 8.7252e+02, 1.7930e+03],\n",
       "        [7.5504e+02, 1.4308e+03, 9.1312e+02, 1.6254e+03],\n",
       "        [2.3796e+03, 3.9763e+02, 2.5396e+03, 5.9336e+02],\n",
       "        [4.7994e+02, 9.4212e+02, 1.1028e+03, 1.7013e+03],\n",
       "        [2.2719e+03, 7.7852e+02, 2.4999e+03, 1.3151e+03],\n",
       "        [1.1119e+03, 8.9340e+01, 1.7368e+03, 8.4786e+02],\n",
       "        [1.9131e+03, 1.6033e+03, 2.1413e+03, 2.1315e+03],\n",
       "        [8.4129e+02, 2.0963e+02, 1.0684e+03, 7.4288e+02],\n",
       "        [4.8441e+02, 1.2597e+03, 7.0851e+02, 1.7956e+03],\n",
       "        [2.3237e+03, 1.8604e+03, 2.4828e+03, 2.0537e+03],\n",
       "        [2.3096e+03, 2.4123e+03, 2.5393e+03, 2.9427e+03],\n",
       "        [5.7983e+02, 1.3765e+03, 1.0270e+03, 2.4575e+03],\n",
       "        [6.3826e+02, 1.4039e+03, 8.6776e+02, 1.9466e+03],\n",
       "        [9.1875e+02, 1.2571e+03, 1.1467e+03, 1.7982e+03],\n",
       "        [3.1170e+02, 1.5051e+03, 9.5035e+02, 2.2780e+03],\n",
       "        [5.6108e+02, 1.2610e+03, 7.8801e+02, 1.7931e+03],\n",
       "        [7.5849e+02, 1.6423e+03, 9.8800e+02, 2.1811e+03],\n",
       "        [1.7944e+03, 1.5098e+03, 2.0241e+03, 2.0373e+03],\n",
       "        [2.3206e+03, 4.2352e+02, 2.4801e+03, 6.1992e+02],\n",
       "        [2.6166e+00, 5.4293e+02, 9.6548e+02, 2.0779e+03],\n",
       "        [1.2370e+03, 3.6332e+02, 1.4630e+03, 8.9245e+02],\n",
       "        [2.2819e+03, 4.4701e+02, 2.4418e+03, 6.4517e+02],\n",
       "        [2.2843e+03, 1.6459e+03, 2.4427e+03, 1.8401e+03],\n",
       "        [2.2611e+03, 3.7499e+02, 2.4224e+03, 5.7332e+02],\n",
       "        [1.4381e+03, 4.5160e+02, 1.6662e+03, 9.8522e+02],\n",
       "        [2.3511e+03, 8.2981e+02, 2.5440e+03, 1.3653e+03],\n",
       "        [2.3408e+03, 4.7193e+02, 2.5008e+03, 6.6822e+02],\n",
       "        [3.2938e+02, 1.7296e+00, 1.5806e+03, 9.3043e+02],\n",
       "        [2.2016e+03, 4.0038e+02, 2.3615e+03, 5.9778e+02],\n",
       "        [1.1569e+03, 3.5950e+02, 1.3846e+03, 8.8782e+02],\n",
       "        [1.6759e+03, 1.6478e+03, 1.9063e+03, 2.1839e+03],\n",
       "        [2.2647e+03, 2.0534e+03, 2.4239e+03, 2.2466e+03],\n",
       "        [7.6192e+02, 2.0893e+02, 9.8783e+02, 7.4351e+02],\n",
       "        [1.9932e+03, 4.4753e+02, 2.2214e+03, 9.8330e+02],\n",
       "        [8.7796e+02, 1.4468e+03, 1.1062e+03, 1.9854e+03],\n",
       "        [1.3173e+03, 5.4849e+02, 1.5452e+03, 1.0769e+03],\n",
       "        [2.2312e+03, 1.8342e+03, 2.4600e+03, 2.3723e+03],\n",
       "        [2.4816e+00, 5.0358e+00, 6.1049e+02, 1.6502e+03],\n",
       "        [2.3115e+03, 5.3992e+02, 2.5393e+03, 1.0781e+03],\n",
       "        [2.3097e+03, 1.7823e+03, 2.5385e+03, 2.3198e+03],\n",
       "        [2.2407e+03, 4.7248e+02, 2.4015e+03, 6.6975e+02],\n",
       "        [1.6678e+03, 2.8313e+02, 2.3037e+03, 1.0497e+03],\n",
       "        [2.3490e+03, 2.2628e+03, 2.5440e+03, 2.7988e+03],\n",
       "        [8.3521e+02, 1.5480e+03, 1.0657e+03, 2.0852e+03],\n",
       "        [2.1525e+03, 1.3075e+03, 2.3811e+03, 1.8532e+03],\n",
       "        [2.1825e+03, 3.7592e+02, 2.3412e+03, 5.7412e+02],\n",
       "        [4.7917e+02, 9.3937e+02, 1.1068e+03, 1.7049e+03],\n",
       "        [6.3453e+02, 1.6436e+03, 7.9435e+02, 1.8398e+03],\n",
       "        [5.5347e+02, 1.3256e+03, 1.1852e+03, 2.0891e+03],\n",
       "        [2.1930e+03, 7.3452e+02, 2.4209e+03, 1.2709e+03],\n",
       "        [4.8292e+02, 1.3986e+03, 7.0814e+02, 1.9440e+03],\n",
       "        [2.2282e+03, 1.5531e+03, 2.4589e+03, 2.0957e+03],\n",
       "        [2.0733e+03, 1.2098e+03, 2.3005e+03, 1.7512e+03],\n",
       "        [4.4312e+02, 1.6354e+03, 6.7010e+02, 2.1815e+03],\n",
       "        [5.3547e+02, 1.4298e+03, 6.9320e+02, 1.6223e+03],\n",
       "        [1.3181e+03, 3.6064e+02, 1.5444e+03, 8.8955e+02],\n",
       "        [1.6343e+03, 6.4374e+02, 1.8628e+03, 1.1734e+03],\n",
       "        [5.1498e+02, 1.4551e+03, 6.7464e+02, 1.6472e+03],\n",
       "        [3.5858e+02, 1.7396e+03, 5.1744e+02, 1.9360e+03],\n",
       "        [2.3224e+03, 3.5157e+02, 2.4788e+03, 5.4912e+02],\n",
       "        [1.9541e+03, 1.2138e+03, 2.1811e+03, 1.7533e+03],\n",
       "        [4.9522e+02, 1.3834e+03, 6.5458e+02, 1.5761e+03],\n",
       "        [2.3783e+03, 5.1763e+02, 2.5388e+03, 7.1708e+02],\n",
       "        [3.6111e+02, 1.6387e+03, 5.8725e+02, 2.1833e+03],\n",
       "        [1.2752e+03, 2.1378e+00, 1.8999e+03, 7.5854e+02],\n",
       "        [4.1824e+02, 1.6903e+03, 5.7598e+02, 1.8884e+03],\n",
       "        [8.7797e+02, 1.1185e+03, 1.1080e+03, 1.6584e+03],\n",
       "        [5.1634e+02, 1.5493e+03, 6.7491e+02, 1.7420e+03],\n",
       "        [6.5436e+02, 1.4301e+03, 8.1414e+02, 1.6242e+03],\n",
       "        [1.3233e+03, 2.7164e+02, 1.6372e+03, 6.5810e+02],\n",
       "        [1.7162e+03, 1.8324e+03, 1.9459e+03, 2.3705e+03],\n",
       "        [4.9756e+02, 1.5260e+03, 6.5648e+02, 1.7194e+03],\n",
       "        [2.3794e+03, 3.2539e+02, 2.5395e+03, 5.2068e+02],\n",
       "        [2.1517e+03, 1.4596e+03, 2.3794e+03, 1.9972e+03]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]['boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 1, 4, 4, 4, 4, 4, 1, 1, 4, 1, 1, 4, 1, 1, 4, 4, 4, 4, 1, 1, 4, 4, 4,\n",
       "        1, 4, 1, 1, 5, 4, 4, 4, 4, 4, 4, 1, 4, 4, 1, 1, 1, 4, 1, 4, 5, 1, 4, 5,\n",
       "        1, 5, 4, 4, 5, 4, 5, 4, 1, 1, 4, 4, 1, 4, 1, 4, 1, 1, 5, 1, 4, 1, 1, 1,\n",
       "        1, 1, 4, 4, 1, 1, 1, 1, 4, 4, 4, 1, 1, 1, 1, 4, 5, 1, 4, 1, 1, 4, 1, 4,\n",
       "        1, 1, 5, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1763, 0.1745, 0.1740, 0.1740, 0.1736, 0.1734, 0.1733, 0.1732, 0.1732,\n",
       "        0.1731, 0.1731, 0.1731, 0.1730, 0.1727, 0.1727, 0.1727, 0.1726, 0.1725,\n",
       "        0.1725, 0.1723, 0.1723, 0.1722, 0.1722, 0.1721, 0.1721, 0.1720, 0.1720,\n",
       "        0.1720, 0.1720, 0.1719, 0.1719, 0.1719, 0.1719, 0.1719, 0.1718, 0.1717,\n",
       "        0.1717, 0.1716, 0.1716, 0.1715, 0.1715, 0.1715, 0.1714, 0.1714, 0.1714,\n",
       "        0.1713, 0.1713, 0.1713, 0.1713, 0.1713, 0.1713, 0.1713, 0.1712, 0.1712,\n",
       "        0.1712, 0.1712, 0.1712, 0.1711, 0.1711, 0.1711, 0.1711, 0.1710, 0.1710,\n",
       "        0.1710, 0.1710, 0.1710, 0.1710, 0.1710, 0.1710, 0.1710, 0.1710, 0.1709,\n",
       "        0.1709, 0.1709, 0.1709, 0.1709, 0.1709, 0.1708, 0.1708, 0.1708, 0.1708,\n",
       "        0.1708, 0.1708, 0.1708, 0.1708, 0.1708, 0.1708, 0.1708, 0.1708, 0.1708,\n",
       "        0.1707, 0.1707, 0.1707, 0.1707, 0.1707, 0.1707, 0.1707, 0.1706, 0.1706,\n",
       "        0.1706], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a402e4e4296f2d4bed1c089fb7c7e828933dcbfe50698b381e393c052eea855"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
