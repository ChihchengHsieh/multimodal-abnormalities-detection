{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out our model here.\n",
    "\n",
    "We test our mutli-modal Faster R-CNN with MIMIC dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "import pandas as pd\n",
    "\n",
    "from data.dataset import ReflacxDataset, collate_fn\n",
    "from utils.transforms import get_transform\n",
    "from models.load import ModelSetup, create_model_from_setup\n",
    "\n",
    "## Suppress the assignement warning from pandas.\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook will running on device: [CUDA]\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "# torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = 'cuda' if use_gpu else 'cpu'\n",
    "print(f\"This notebook will running on device: [{device.upper()}]\")\n",
    "\n",
    "if use_gpu:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your MIMIC folde path here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "XAMI_MIMIC_PATH = \"D:\\XAMI-MIMIC\"\n",
    "\n",
    "use_iobb = True\n",
    "io_type_str = \"IoBB\" if use_iobb else \"IoU\"\n",
    "\n",
    "all_model_setups = [\n",
    "    ModelSetup(\n",
    "        name=\"original\",\n",
    "        use_clinical=False,\n",
    "        use_custom_model=False,\n",
    "        use_early_stop_model=True,\n",
    "    ),\n",
    "    ModelSetup(\n",
    "        name=\"custom_without_clinical\",\n",
    "        use_clinical=False,\n",
    "        use_custom_model=True,\n",
    "        use_early_stop_model=True,\n",
    "    ),\n",
    "    ModelSetup(\n",
    "        name=\"custom_with_clinical\",\n",
    "        use_clinical=True,\n",
    "        use_custom_model=True,\n",
    "        use_early_stop_model=True,\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_setup = all_model_setups[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate datasets and dataloaders\n",
    "The batch size is also defined in this section. For testing purpose, we only set it as 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_cols = [\n",
    "    \"Enlarged cardiac silhouette\",\n",
    "    \"Atelectasis\",\n",
    "    \"Pleural abnormality\",\n",
    "    \"Consolidation\",\n",
    "    \"Pulmonary edema\",\n",
    "    #  'Groundglass opacity', # 6th disease.\n",
    "]\n",
    "\n",
    "dataset_params_dict = {\n",
    "    \"XAMI_MIMIC_PATH\": XAMI_MIMIC_PATH,\n",
    "    \"with_clinical\": model_setup.use_clinical,\n",
    "    \"dataset_mode\": \"normal\",\n",
    "    \"bbox_to_mask\": True,\n",
    "    \"labels_cols\": labels_cols,\n",
    "}\n",
    "\n",
    "detect_eval_dataset = ReflacxDataset(\n",
    "    **{**dataset_params_dict, \"dataset_mode\": \"normal\",},\n",
    "    transforms=get_transform(train=False),\n",
    ")\n",
    "\n",
    "train_dataset = ReflacxDataset(\n",
    "    **dataset_params_dict, split_str=\"train\", transforms=get_transform(train=True),\n",
    ")\n",
    "\n",
    "val_dataset = ReflacxDataset(\n",
    "    **dataset_params_dict, split_str=\"val\", transforms=get_transform(train=False),\n",
    ")\n",
    "\n",
    "test_dataset = ReflacxDataset(\n",
    "    **dataset_params_dict, split_str=\"test\", transforms=get_transform(train=False),\n",
    ")\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We used to have 670, after unifying, we will have 590.\n"
     ]
    }
   ],
   "source": [
    "print(f\"We used to have {len(detect_eval_dataset.df.dicom_id)}, after unifying, we will have {len(detect_eval_dataset.df.dicom_id.unique())}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example instance from dataset\n",
    "We show what's inside a single instance. It will provide:\n",
    "\n",
    "- Images\n",
    "- Clinical data\n",
    "- Targets (Dictionary)\n",
    "\n",
    "And, inside the target, there're:\n",
    "\n",
    "- boxes (bounding boxes of abnormality)\n",
    "- lable (disease index (Note: the class **0** means the background))\n",
    "- image_id (idx to get that image)\n",
    "- area (the areas that bouding boxes contain)\n",
    "- iscrowd (if it's a place with multiple bouding boxes, we assume all the the bouding boxes are not crowd.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model.\n",
    "\n",
    "We define he models here. Two backbone examples are in the below code section. The MobileNet is a light weight network, and ResNet is heavier, but usually perform better. In our case, the calculation is not the most important factor; therefore, we chose ResNet with feature pyramid networks (FPN) backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rpn_nms_thresh': 0.3, 'box_detections_per_img': 6, 'box_nms_thresh': 0.2, 'rpn_score_thresh': 0.0, 'box_score_thresh': 0.05}\n",
      "c1\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = create_model_from_setup(detect_eval_dataset, model_setup, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data to feed\n",
    "\n",
    "We prepare three main data to test the model:\n",
    "\n",
    "- CXR image\n",
    "- Clinical data\n",
    "- Target\n",
    "\n",
    "And, for each data, we adjust the format to what the model expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using unified.\n",
      "Using unified.\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_dataloader))\n",
    "data = train_dataset.prepare_input_from_data(data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0', dtype=torch.uint8),\n",
       "  'image_path': 'D:\\\\XAMI-MIMIC\\\\patient_11495809\\\\CXR-JPG\\\\s52705409\\\\0295a5c7-982330bd-2203b511-4d052b0f-a43a5e17.jpg',\n",
       "  'dicom_id': '0295a5c7-982330bd-2203b511-4d052b0f-a43a5e17',\n",
       "  'iscrowd': tensor([0, 0], device='cuda:0'),\n",
       "  'area': tensor([303715., 707805.], device='cuda:0', dtype=torch.float64),\n",
       "  'image_id': tensor([108], device='cuda:0'),\n",
       "  'labels': tensor([2, 2], device='cuda:0'),\n",
       "  'boxes': tensor([[1719., 1623., 2414., 2060.],\n",
       "          [ 391., 1392., 1336., 2141.]], device='cuda:0', dtype=torch.float64)},\n",
       " {'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0', dtype=torch.uint8),\n",
       "  'image_path': 'D:\\\\XAMI-MIMIC\\\\patient_11287042\\\\CXR-JPG\\\\s50657073\\\\1ad21961-ee94488b-7fc68fbd-3a8a8100-9b71edfc.jpg',\n",
       "  'dicom_id': '1ad21961-ee94488b-7fc68fbd-3a8a8100-9b71edfc',\n",
       "  'iscrowd': tensor([0], device='cuda:0'),\n",
       "  'area': tensor([132561.], device='cuda:0', dtype=torch.float64),\n",
       "  'image_id': tensor([89], device='cuda:0'),\n",
       "  'labels': tensor([3], device='cuda:0'),\n",
       "  'boxes': tensor([[ 340., 1488.,  649., 1917.]], device='cuda:0', dtype=torch.float64)}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Feedforawrd (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mike8\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "output = model(*data[:-1], targets=data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results we get.\n",
    "Four different losses are given in the result, we will use these losses to optimise the network while training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_classifier': tensor(1.8260, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " 'loss_box_reg': tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'loss_mask': tensor(0.8756, device='cuda:0',\n",
       "        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " 'loss_objectness': tensor(0.7195, device='cuda:0',\n",
       "        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " 'loss_rpn_box_reg': tensor(0.0047, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Feedforawrd (Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred = model(*data[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results we get.\n",
    "If we set the model to evaluation mode and don't pass the target to the forward function, the model will output prediction (detections). In the below sections, we show what's inside the detection of first instance (idx=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection.\n",
    "\n",
    "A detection contain *boxes*, *lables*, and *scores*.\n",
    "\n",
    "- *boxes*: All the bounding boxes for this image. \n",
    "- *lables*: Labels corresponded to the bounding boxes.\n",
    "- *score*: Score (Confidence) for each boudning box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['boxes', 'labels', 'scores', 'masks'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 468.9502, 1203.1854,  567.1108, 1426.9713],\n",
       "        [2325.5161, 1316.1874, 2524.6357, 1799.9176],\n",
       "        [2417.6809,  602.9283, 2519.1047,  829.5873],\n",
       "        [ 532.8803, 1454.3362,  625.7414, 1654.4663],\n",
       "        [   0.0000, 1011.0303, 1005.5333, 2017.4100],\n",
       "        [   0.0000,   35.5040,  140.0621,  504.7518]], device='cuda:0',\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]['boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 3, 4, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1922, 0.1879, 0.1859, 0.1859, 0.1854, 0.1841], device='cuda:0',\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a402e4e4296f2d4bed1c089fb7c7e828933dcbfe50698b381e393c052eea855"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
