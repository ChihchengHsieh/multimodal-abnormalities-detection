{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out our model here.\n",
    "\n",
    "We test our mutli-modal Faster R-CNN with MIMIC dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mike8\\Desktop\\multimodal-abnormalities-detection\\models\\detectors\\rcnn.py:839: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import utils.print as print_f\n",
    "\n",
    "from utils.coco_eval import get_eval_params_dict\n",
    "from utils.engine import xami_train_one_epoch, xami_evaluate, get_iou_types\n",
    "from utils.plot import plot_losses, plot_train_val_ap_ars, get_ap_ar_for_train_val\n",
    "from utils.save import get_data_from_metric_logger\n",
    "from utils.coco_utils import get_cocos\n",
    "\n",
    "from models.setup import ModelSetup\n",
    "from models.build import create_model_from_setup\n",
    "from models.train import TrainingInfo\n",
    "from utils.save import check_best, end_train\n",
    "from data.load import get_datasets, get_dataloaders\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from utils.eval import get_ar_ap\n",
    "from utils.train import get_optimiser, get_lr_scheduler, print_params_setup\n",
    "from utils.init import reproducibility, clean_memory_get_device\n",
    "from data.constants import DEFAULT_REFLACX_LABEL_COLS, XAMI_MIMIC_PATH\n",
    "from  datetime import datetime\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "\n",
    "## Suppress the assignement warning from pandas.r\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "## Supress user warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook will running on device: [CUDA]\n"
     ]
    }
   ],
   "source": [
    "device = clean_memory_get_device()\n",
    "reproducibility()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your MIMIC folde path here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_iobb = True\n",
    "io_type_str = \"IoBB\" if use_iobb else \"IoU\"\n",
    "labels_cols = DEFAULT_REFLACX_LABEL_COLS\n",
    "iou_thrs = np.array([0.5])\n",
    "\n",
    "\n",
    "model_setup =  ModelSetup(\n",
    "        name=\"CXR_Clinical\",\n",
    "        use_clinical=True,\n",
    "        use_custom_model=True,\n",
    "        use_early_stop_model=True,\n",
    "        best_ar_val_model_path=None,\n",
    "        best_ap_val_model_path=None,\n",
    "        final_model_path=None,\n",
    "        backbone=\"resnet50\",\n",
    "        optimiser=\"sgd\",\n",
    "        lr=1e-2,\n",
    "        # lr=1e-4,\n",
    "        # weight_decay=0.001,\n",
    "        weight_decay=0,\n",
    "        pretrained=True,\n",
    "        record_training_performance=True,\n",
    "        dataset_mode=\"unified\",\n",
    "        image_size=256,\n",
    "        backbone_out_channels=16,\n",
    "        batch_size=4,\n",
    "        warmup_epochs=0,\n",
    "        lr_scheduler=\"ReduceLROnPlateau\",\n",
    "        # lr_scheduler=None,\n",
    "        reduceLROnPlateau_factor=0.1,\n",
    "        reduceLROnPlateau_patience=10,\n",
    "        reduceLROnPlateau_full_stop=False,\n",
    "        multiStepLR_milestones=[30, 50, 70, 90],\n",
    "        multiStepLR_gamma=0.1,\n",
    "        representation_size=32,\n",
    "        mask_hidden_layers=256,\n",
    "        using_fpn=True,\n",
    "        use_mask=False,\n",
    "        clinical_expand_dropout_rate=0,\n",
    "        clinical_conv_dropout_rate=0,\n",
    "        clinical_input_channels=32,\n",
    "        clinical_num_len=9,\n",
    "        clinical_conv_channels=256,\n",
    "        fuse_conv_channels=32,\n",
    "        fuse_dropout_rate=0,\n",
    "        box_head_dropout_rate=0,\n",
    "        fuse_depth=4,\n",
    "        fusion_strategy=\"add\",\n",
    "        fusion_residule=False,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate datasets and dataloaders\n",
    "The batch size is also defined in this section. For testing purpose, we only set it as 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params_dict = {\n",
    "    \"XAMI_MIMIC_PATH\": XAMI_MIMIC_PATH,\n",
    "    \"with_clinical\": model_setup.use_clinical,\n",
    "    \"dataset_mode\": model_setup.dataset_mode,\n",
    "    \"bbox_to_mask\": model_setup.use_mask,\n",
    "    \"labels_cols\": DEFAULT_REFLACX_LABEL_COLS,\n",
    "}\n",
    "\n",
    "detect_eval_dataset, train_dataset, val_dataset, test_dataset = get_datasets(\n",
    "    dataset_params_dict=dataset_params_dict\n",
    ")\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = get_dataloaders(\n",
    "    train_dataset, val_dataset, test_dataset, batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We used to have 590, after unifying, we will have 590.\n"
     ]
    }
   ],
   "source": [
    "print(f\"We used to have {len(detect_eval_dataset.df.dicom_id)}, after unifying, we will have {len(detect_eval_dataset.df.dicom_id.unique())}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example instance from dataset\n",
    "We show what's inside a single instance. It will provide:\n",
    "\n",
    "- Images\n",
    "- Clinical data\n",
    "- Targets (Dictionary)\n",
    "\n",
    "And, inside the target, there're:\n",
    "\n",
    "- boxes (bounding boxes of abnormality)\n",
    "- lable (disease index (Note: the class **0** means the background))\n",
    "- image_id (idx to get that image)\n",
    "- area (the areas that bouding boxes contain)\n",
    "- iscrowd (if it's a place with multiple bouding boxes, we assume all the the bouding boxes are not crowd.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.5608, 0.5608, 0.5569,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5569, 0.5647, 0.5647,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5490, 0.5569, 0.5608,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.7686, 0.7686, 0.7647,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.7608, 0.7647, 0.7608,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.7529, 0.7569, 0.7569,  ..., 1.0000, 1.0000, 1.0000]],\n",
       " \n",
       "         [[0.5608, 0.5608, 0.5569,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5569, 0.5647, 0.5647,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5490, 0.5569, 0.5608,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.7686, 0.7686, 0.7647,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.7608, 0.7647, 0.7608,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.7529, 0.7569, 0.7569,  ..., 1.0000, 1.0000, 1.0000]],\n",
       " \n",
       "         [[0.5608, 0.5608, 0.5569,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5569, 0.5647, 0.5647,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5490, 0.5569, 0.5608,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.7686, 0.7686, 0.7647,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.7608, 0.7647, 0.7608,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.7529, 0.7569, 0.7569,  ..., 1.0000, 1.0000, 1.0000]]]),\n",
       " tensor([ 69.0000,  98.1000,  90.0000,  18.0000,  99.0000, 184.0000,  75.0000,\n",
       "          13.0000,   3.0000]),\n",
       " tensor([0], dtype=torch.int32),\n",
       " {'boxes': tensor([[ 734., 1204., 2211., 2175.],\n",
       "          [ 831., 1204., 2223., 2310.],\n",
       "          [ 692., 1364., 2257., 2261.],\n",
       "          [ 887., 1081., 2248., 2344.]], dtype=torch.float64),\n",
       "  'labels': tensor([1, 1, 1, 1]),\n",
       "  'image_id': tensor([0]),\n",
       "  'area': tensor([1434167., 1539552., 1403805., 1718943.], dtype=torch.float64),\n",
       "  'iscrowd': tensor([0, 0, 0, 0]),\n",
       "  'dicom_id': '34cedb74-d0996b40-6d218312-a9174bea-d48dc033',\n",
       "  'image_path': 'D:\\\\XAMI-MIMIC\\\\patient_18111516\\\\CXR-JPG\\\\s55032240\\\\34cedb74-d0996b40-6d218312-a9174bea-d48dc033.jpg'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model.\n",
    "\n",
    "We define he models here. Two backbone examples are in the below code section. The MobileNet is a light weight network, and ResNet is heavier, but usually perform better. In our case, the calculation is not the most important factor; therefore, we chose ResNet with feature pyramid networks (FPN) backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load custom model\n",
      "Using ResNet as backbone\n",
      "Using pretrained backbone. resnet50\n",
      "Not using pretrained MaksRCNN model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultimodalMaskRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): XAMIRegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): XAMIRoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): XAMITwoMLPHead(\n",
       "      (fc6): Sequential(\n",
       "        (0): Linear(in_features=12544, out_features=32, bias=True)\n",
       "        (1): Dropout2d(p=0, inplace=False)\n",
       "      )\n",
       "      (fc7): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): Dropout2d(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=32, out_features=6, bias=True)\n",
       "      (bbox_pred): Linear(in_features=32, out_features=24, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (gender_emb_layer): Embedding(2, 23)\n",
       "  (clinical_expand_conv): Sequential(\n",
       "    (0): ConvTranspose2d(32, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout2d(p=0, inplace=False)\n",
       "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout2d(p=0, inplace=False)\n",
       "    (8): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout2d(p=0, inplace=False)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "    (15): Dropout2d(p=0, inplace=False)\n",
       "    (16): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): ReLU()\n",
       "    (19): Dropout2d(p=0, inplace=False)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU()\n",
       "    (23): Dropout2d(p=0, inplace=False)\n",
       "    (24): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU()\n",
       "    (27): Dropout2d(p=0, inplace=False)\n",
       "    (28): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (30): ReLU()\n",
       "    (31): Dropout2d(p=0, inplace=False)\n",
       "    (32): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (33): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (34): ReLU()\n",
       "    (35): Dropout2d(p=0, inplace=False)\n",
       "    (36): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (37): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (38): ReLU()\n",
       "    (39): Dropout2d(p=0, inplace=False)\n",
       "    (40): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (41): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU()\n",
       "    (43): Dropout2d(p=0, inplace=False)\n",
       "    (44): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (45): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (46): ReLU()\n",
       "    (47): Dropout2d(p=0, inplace=False)\n",
       "    (48): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (49): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (50): ReLU()\n",
       "    (51): Dropout2d(p=0, inplace=False)\n",
       "    (52): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (53): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (54): ReLU()\n",
       "    (55): Dropout2d(p=0, inplace=False)\n",
       "    (56): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (57): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (58): ReLU()\n",
       "    (59): Dropout2d(p=0, inplace=False)\n",
       "    (60): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (61): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (clinical_convs): ModuleDict(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (pool): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (fuse_convs): ModuleDict(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (pool): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model_from_setup(\n",
    "    labels_cols,\n",
    "    model_setup,\n",
    "    rpn_nms_thresh=0.3,\n",
    "    box_detections_per_img=10,\n",
    "    box_nms_thresh=0.2,\n",
    "    rpn_score_thresh=0.0,\n",
    "    box_score_thresh=0.05,\n",
    "    # image_size=model_setup.image_size,\n",
    "    # clinical_conv_channels=64,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[model]: 40,891,931\n",
      "[model.backbone]: 26,799,296\n",
      "[model.rpn]: 593,935\n",
      "[model.roi_heads]: 403,486\n",
      "[model.roi_heads.box_head]: 402,496\n",
      "[model.roi_heads.box_head.fc6]: 401,440\n",
      "[model.roi_heads.box_head.fc7]: 1,056\n",
      "[model.roi_heads.box_predictor]: 990\n",
      "[model.clinical_convs]: 3,543,552\n",
      "[model.fuse_convs]: 2,952,960\n"
     ]
    }
   ],
   "source": [
    "from utils.train import print_params_setup\n",
    "print_params_setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data to feed\n",
    "\n",
    "We prepare three main data to test the model:\n",
    "\n",
    "- CXR image\n",
    "- Clinical data\n",
    "- Target\n",
    "\n",
    "And, for each data, we adjust the format to what the model expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_dataloader))\n",
    "data = train_dataset.prepare_input_from_data(data, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Feedforawrd (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = [gt_label[idxs] for gt_label, idxs in zip(gt_labels, mask_matched_idxs)]\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 3, 3], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1][0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, targets = data\n",
    "# images, targets = model.transform(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "loss_dict, outputs = model(*data[:-1], targets=data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_classifier': tensor(1.8202, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " 'loss_box_reg': tensor(0.0120, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'loss_objectness': tensor(0.6944, device='cuda:0',\n",
       "        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " 'loss_rpn_box_reg': tensor(0.0041, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[ 727.8102, 1637.8724,  986.2892, 2099.2532],\n",
       "          [1223.2861, 2047.7300, 1629.6429, 2353.9402],\n",
       "          [1358.6228, 1034.7659, 1909.6292, 1239.0436],\n",
       "          [1039.1136, 2289.4778, 1447.1873, 2544.0000],\n",
       "          [ 610.3279, 1914.5939,  878.5331, 2401.5198],\n",
       "          [   0.0000, 1394.8024,  927.4928, 1840.9017],\n",
       "          [1195.2316, 1532.8962, 1458.2839, 1984.2397],\n",
       "          [   0.0000, 1599.1731,  126.5366, 2048.1160],\n",
       "          [2754.4048,   87.3961, 3048.7983,  395.0801],\n",
       "          [2259.4878,  775.1384, 2637.0986, 1098.3540]], device='cuda:0',\n",
       "         grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0'),\n",
       "  'scores': tensor([0.2139, 0.2126, 0.2122, 0.2118, 0.2110, 0.2086, 0.2084, 0.2080, 0.2079,\n",
       "          0.2078], device='cuda:0', grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([[ 281.3226, 2651.7107,  510.8072, 3056.0000],\n",
       "          [ 447.5430, 2615.5415,  671.3224, 3056.0000],\n",
       "          [2114.5415, 2726.0354, 2332.2610, 3056.0000],\n",
       "          [2317.4902, 2460.3237, 2536.3303, 2852.1555],\n",
       "          [ 677.7936, 2113.8083,  885.4900, 2669.6272],\n",
       "          [ 717.7136, 2442.4573, 1064.5582, 2872.4348],\n",
       "          [   0.0000, 2112.9487,   95.5255, 2684.7979],\n",
       "          [ 336.2043, 1515.1053,  778.5378, 1773.0453],\n",
       "          [  46.3993, 2144.3132,  261.4251, 2683.9070],\n",
       "          [   0.0000, 2706.7388,   96.8838, 3056.0000]], device='cuda:0',\n",
       "         grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0'),\n",
       "  'scores': tensor([0.2339, 0.2319, 0.2301, 0.2253, 0.2228, 0.2219, 0.2203, 0.2202, 0.2186,\n",
       "          0.2165], device='cuda:0', grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([[1.7170e+03, 3.7311e+02, 2.1416e+03, 6.2440e+02],\n",
       "          [3.7666e+02, 2.7630e+03, 6.9900e+02, 3.0560e+03],\n",
       "          [1.7479e+03, 2.7526e+03, 2.0570e+03, 3.0560e+03],\n",
       "          [1.8696e+03, 2.0085e+03, 2.0593e+03, 2.6304e+03],\n",
       "          [1.5875e+03, 5.0699e+00, 1.9056e+03, 3.9050e+02],\n",
       "          [1.1798e+03, 2.8367e+03, 1.6333e+03, 3.0560e+03],\n",
       "          [2.0446e+03, 1.7137e+03, 2.2474e+03, 2.2799e+03],\n",
       "          [9.2346e+01, 2.6367e+03, 5.3463e+02, 2.9234e+03],\n",
       "          [1.3334e+03, 5.7092e+02, 2.0019e+03, 1.4142e+03],\n",
       "          [1.4548e+00, 2.7596e+03, 1.7497e+02, 3.0560e+03]], device='cuda:0',\n",
       "         grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0'),\n",
       "  'scores': tensor([0.2284, 0.2240, 0.2207, 0.2195, 0.2189, 0.2180, 0.2180, 0.2156, 0.2146,\n",
       "          0.2142], device='cuda:0', grad_fn=<IndexBackward0>)},\n",
       " {'boxes': tensor([[1498.9850, 2055.8579, 2307.7666, 2544.0000],\n",
       "          [ 662.9584, 2302.2271, 1045.9449, 2544.0000],\n",
       "          [1326.1584, 2294.8145, 1712.7277, 2544.0000],\n",
       "          [ 557.5967,  919.3446,  799.4056, 1437.8444],\n",
       "          [1122.7145, 1542.5791, 1369.2592, 2016.5519],\n",
       "          [ 943.4504, 2304.8660, 1329.6716, 2544.0000],\n",
       "          [2835.7224,  900.3586, 3050.5530, 1370.7828],\n",
       "          [ 317.7783, 1879.8223,  847.1580, 2110.7805],\n",
       "          [2359.6638, 2022.7767, 2612.2026, 2513.0679],\n",
       "          [2150.4634, 2107.3271, 2405.5410, 2544.0000]], device='cuda:0',\n",
       "         grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0'),\n",
       "  'scores': tensor([0.2345, 0.2256, 0.2239, 0.2228, 0.2202, 0.2183, 0.2182, 0.2181, 0.2160,\n",
       "          0.2157], device='cuda:0', grad_fn=<IndexBackward0>)}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]: torch.Size([4, 256, 64, 64])\n",
      "[1]: torch.Size([4, 256, 32, 32])\n",
      "[2]: torch.Size([4, 256, 16, 16])\n",
      "[3]: torch.Size([4, 256, 8, 8])\n",
      "[pool]: torch.Size([4, 256, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "for k, v in model.img_features.items():\n",
    "    print(f\"[{k}]: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]: torch.Size([4, 256, 64, 64])\n",
      "[1]: torch.Size([4, 256, 32, 32])\n",
      "[2]: torch.Size([4, 256, 16, 16])\n",
      "[3]: torch.Size([4, 256, 8, 8])\n",
      "[pool]: torch.Size([4, 256, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "for k, v in model.clinical_features.items():\n",
    "    print(f\"[{k}]: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 64, 64])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clinical_features['0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 32, 32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fuse_convs['0'](model.clinical_features['0']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_features = OrderedDict({})\n",
    "\n",
    "clinical_input = model.clinical_features['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 64, 64])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinical_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['0', '1', '2', '3', 'pool'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clinical_convs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clinical_convs['2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_input = model.clinical_convs['0'](clinical_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 16, 16])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinical_input.shape # shrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_features[k] = clinical_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 64, 64])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clinical_features['0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pool]: torch.Size([4, 256, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "for k, v in clinical_features.items():\n",
    "    print(f\"[{k}]: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clinical_features = OrderedDict({})\n",
    "\n",
    "clinical_input = model.clinical_features['0']\n",
    "\n",
    "for k in model.clinical_convs.keys():\n",
    "    clinical_input = model.clinical_convs[k](clinical_input)\n",
    "    clinical_features[k] = clinical_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 1, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinical_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]: torch.Size([4, 256, 16, 16])\n",
      "[1]: torch.Size([4, 256, 8, 8])\n",
      "[2]: torch.Size([4, 256, 4, 4])\n",
      "[3]: torch.Size([4, 256, 2, 2])\n",
      "[pool]: torch.Size([4, 256, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for k, v in clinical_features.items():\n",
    "    print(f\"[{k}]: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_input = model.clinical_convs['0'](clinical_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinical_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_features[k] = clinical_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 1, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinical_features[k].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3652, 0.0000, 0.0000, 0.4546, 0.0000, 0.8485, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.2708, 0.0000, 0.0000],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinical_features['0'][-1][-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.7062, 0.0000, 0.0000, 0.0000],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinical_features['1'][-1][-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]: torch.Size([4, 256, 64, 64])\n",
      "[1]: torch.Size([4, 256, 32, 32])\n",
      "[2]: torch.Size([4, 256, 16, 16])\n",
      "[3]: torch.Size([4, 256, 8, 8])\n",
      "[pool]: torch.Size([4, 256, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "for k, v in model.img_features.items():\n",
    "    print(f\"[{k}]: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20848/477851150.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "images, targets = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image_sizes= []\n",
    "for img in images:\n",
    "    val = img.shape[-2:]\n",
    "    assert len(val) == 2\n",
    "    original_image_sizes.append((val[0], val[1]))\n",
    "\n",
    "images, targets = model.transform(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_features = model.backbone(images.tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pool'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(img_features.keys())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]: torch.Size([4, 256, 64, 64])\n",
      "[1]: torch.Size([4, 256, 32, 32])\n",
      "[2]: torch.Size([4, 256, 16, 16])\n",
      "[3]: torch.Size([4, 256, 8, 8])\n",
      "[pool]: torch.Size([4, 256, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "for k, v in img_features.items():\n",
    "    print(f\"[{k}]: {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results we get.\n",
    "Four different losses are given in the result, we will use these losses to optimise the network while training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Feedforawrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection.\n",
    "\n",
    "A detection contain *boxes*, *lables*, and *scores*.\n",
    "\n",
    "- *boxes*: All the bounding boxes for this image. \n",
    "- *lables*: Labels corresponded to the bounding boxes.\n",
    "- *score*: Score (Confidence) for each boudning box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'loss_classifier': tensor(1.8701, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       "  'loss_box_reg': tensor(0.0179, device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  'loss_objectness': tensor(0.6847, device='cuda:0',\n",
       "         grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       "  'loss_rpn_box_reg': tensor(0.0115, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)},\n",
       " [{'boxes': tensor([[1.6144e+03, 1.2710e+03, 1.8540e+03, 1.7224e+03],\n",
       "           [1.4235e+03, 1.2276e+03, 1.6649e+03, 1.6887e+03],\n",
       "           [2.5160e+03, 2.3314e+03, 2.9126e+03, 2.5440e+03],\n",
       "           [1.2205e+03, 1.3219e+03, 1.4771e+03, 1.7699e+03],\n",
       "           [7.5401e+02, 2.3299e+03, 1.1488e+03, 2.5440e+03],\n",
       "           [5.5990e+02, 5.8346e-01, 8.1155e+02, 2.4206e+02],\n",
       "           [3.1914e+02, 2.3218e+03, 7.2241e+02, 2.5429e+03],\n",
       "           [1.5083e+03, 1.5982e+03, 2.3042e+03, 2.2397e+03],\n",
       "           [1.5508e+03, 2.0036e+03, 1.8119e+03, 2.4510e+03],\n",
       "           [2.3595e+03, 4.5017e+00, 2.6145e+03, 2.6230e+02]], device='cuda:0',\n",
       "          grad_fn=<StackBackward0>),\n",
       "   'labels': tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0'),\n",
       "   'scores': tensor([0.2126, 0.2080, 0.2075, 0.2073, 0.2072, 0.2043, 0.2038, 0.2033, 0.2031,\n",
       "           0.2030], device='cuda:0', grad_fn=<IndexBackward0>)},\n",
       "  {'boxes': tensor([[1549.6898,   10.4852, 2006.8060,  559.1583],\n",
       "           [1579.2511,    5.4665, 1788.0436,  236.7701],\n",
       "           [ 716.9372,   68.4386, 1066.6201,  453.6077],\n",
       "           [2011.3376,    6.0520, 2367.1765,  362.4406],\n",
       "           [   0.0000,    7.2869,  518.3975,  418.2392],\n",
       "           [1360.8907,  160.7071, 1721.7186,  525.7047],\n",
       "           [1013.2926,    4.3466, 1366.3706,  144.0656],\n",
       "           [1337.2595,    6.4978, 1552.4139,  243.2007],\n",
       "           [  67.6765,   49.2905, 2544.0000, 1306.4609],\n",
       "           [1200.0514, 2589.3716, 1415.4020, 3056.0000]], device='cuda:0',\n",
       "          grad_fn=<StackBackward0>),\n",
       "   'labels': tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0'),\n",
       "   'scores': tensor([0.2338, 0.2223, 0.2213, 0.2210, 0.2197, 0.2178, 0.2161, 0.2156, 0.2153,\n",
       "           0.2147], device='cuda:0', grad_fn=<IndexBackward0>)},\n",
       "  {'boxes': tensor([[  57.8676, 1943.0632,  275.8228, 2453.2534],\n",
       "           [1137.7451, 1074.6543, 1348.3672, 1586.2690],\n",
       "           [ 174.4837, 2123.8367,  398.4944, 2643.5051],\n",
       "           [ 332.8510, 2496.8545,  559.7200, 3019.0608],\n",
       "           [  94.2272, 2455.0237,  321.5002, 2982.1958],\n",
       "           [ 298.8264, 1893.2606,  518.2466, 2405.2937],\n",
       "           [ 581.6752,  807.4842,  803.1099, 1332.5215],\n",
       "           [1021.0121,  806.8638, 1236.9781, 1334.1943],\n",
       "           [1066.7906, 1076.3342, 1800.1696, 1848.8717],\n",
       "           [   0.0000, 2391.9031,  159.4653, 2923.4856]], device='cuda:0',\n",
       "          grad_fn=<StackBackward0>),\n",
       "   'labels': tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0'),\n",
       "   'scores': tensor([0.2324, 0.2267, 0.2261, 0.2259, 0.2257, 0.2246, 0.2236, 0.2221, 0.2218,\n",
       "           0.2216], device='cuda:0', grad_fn=<IndexBackward0>)},\n",
       "  {'boxes': tensor([[ 151.9529,   85.2113,  397.7711,  508.7312],\n",
       "           [ 324.2072,  186.0547,  721.3181,  513.4020],\n",
       "           [2206.7773,   71.8307, 3056.0000, 2064.2812],\n",
       "           [ 565.8337, 1045.8027, 2622.5796, 1916.2834],\n",
       "           [ 449.9645,    9.8993, 1263.7251,  323.7435],\n",
       "           [2119.5505, 1075.4841, 2383.3000, 1511.9158],\n",
       "           [2210.9280,  680.2021, 2757.7483,  895.9147],\n",
       "           [2411.0439, 1692.0981, 2974.9517, 1903.4011],\n",
       "           [2638.8525,  900.7786, 2910.0161, 1345.4674],\n",
       "           [2470.8960,   36.2952, 2869.1760,  350.7276]], device='cuda:0',\n",
       "          grad_fn=<StackBackward0>),\n",
       "   'labels': tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0'),\n",
       "   'scores': tensor([0.2154, 0.2154, 0.2119, 0.2101, 0.2093, 0.2087, 0.2076, 0.2064, 0.2060,\n",
       "           0.2044], device='cuda:0', grad_fn=<IndexBackward0>)}])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_dict, outputs "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52a48fdedee40b77eb251917c5aa239bf02f1ab8c93cc13fe7347f570eadc6b9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
