{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out our model here.\n",
    "\n",
    "We test our mutli-modal Faster R-CNN with MIMIC dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mmcv in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: addict in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from mmcv) (2.4.0)\n",
      "Requirement already satisfied: regex in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from mmcv) (2021.8.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from mmcv) (5.4.1)\n",
      "Requirement already satisfied: yapf in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from mmcv) (0.31.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from mmcv) (1.20.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from mmcv) (21.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from mmcv) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from packaging->mmcv) (3.0.4)\n",
      "Collecting timm\n",
      "  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: torch>=1.4 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from timm) (1.12.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from timm) (0.13.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from torch>=1.4->timm) (3.10.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from torchvision->timm) (8.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from torchvision->timm) (1.20.3)\n",
      "Requirement already satisfied: requests in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from torchvision->timm) (2.26.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from requests->torchvision->timm) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from requests->torchvision->timm) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from requests->torchvision->timm) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from requests->torchvision->timm) (1.26.7)\n",
      "Installing collected packages: timm\n",
      "Successfully installed timm-0.6.7\n"
     ]
    }
   ],
   "source": [
    "!pip install mmcv\n",
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilizador\\Desktop\\TESE\\multimodal-abnormalities-detection\\models\\detectors\\rcnn.py:866: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from models.setup import ModelSetup\n",
    "from models.build import create_model_from_setup\n",
    "from data.load import get_datasets, get_dataloaders\n",
    "\n",
    "from utils.init import reproducibility, clean_memory_get_device\n",
    "from data.constants import DEFAULT_REFLACX_LABEL_COLS, XAMI_MIMIC_PATH\n",
    "\n",
    "## Suppress the assignement warning from pandas.r\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "## Supress user warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook will running on device: [CPU]\n"
     ]
    }
   ],
   "source": [
    "device = clean_memory_get_device()\n",
    "reproducibility()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your MIMIC folde path here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_iobb = True\n",
    "io_type_str = \"IoBB\" if use_iobb else \"IoU\"\n",
    "labels_cols = DEFAULT_REFLACX_LABEL_COLS\n",
    "iou_thrs = np.array([0.5])\n",
    "\n",
    "\n",
    "common_args = {\n",
    "    \"use_custom_model\": True,\n",
    "    \"use_early_stop_model\": True,\n",
    "    \"optimiser\": \"sgd\",\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"pretrained\": True,\n",
    "    \"record_training_performance\": True,\n",
    "    \"dataset_mode\": \"normal\",\n",
    "    \"image_size\": 512,\n",
    "    \"batch_size\": 4,\n",
    "    \"warmup_epochs\": 0,\n",
    "    \"lr_scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"reduceLROnPlateau_factor\": 0.1,\n",
    "    \"reduceLROnPlateau_patience\": 999,\n",
    "    \"reduceLROnPlateau_full_stop\": True,\n",
    "    \"multiStepLR_milestones\": 100,\n",
    "    \"multiStepLR_gamma\": 0.1,\n",
    "    \"use_mask\": True,\n",
    "    \"clinical_num_len\": 9,\n",
    "    \"gt_in_train_till\": 999,\n",
    "    \"box_head_dropout_rate\": 0,\n",
    "    \"spatialise_method\": \"convs\",  # [convs, repeat]\n",
    "    \"normalise_clinical_num\": False,\n",
    "    \"measure_test\": True,\n",
    "}\n",
    "\n",
    "fusion_add_args = {\n",
    "    \"fuse_depth\": 0,\n",
    "    \"fusion_residule\": False,\n",
    "    \"fusion_strategy\": \"add\",\n",
    "}\n",
    "\n",
    "small_model_args = {\n",
    "    \"mask_hidden_layers\": 64,\n",
    "    \"fuse_conv_channels\": 64,\n",
    "    \"clinical_input_channels\": 64,\n",
    "    \"representation_size\": 64,  # 32\n",
    "    \"clinical_conv_channels\": 64,\n",
    "    \"clinical_expand_conv_channels\": 64,\n",
    "    \"backbone_out_channels\": 64,\n",
    "}\n",
    "\n",
    "mobilenet_args = {\n",
    "    \"backbone\": \"mobilenet_v3\",\n",
    "    \"using_fpn\": False,\n",
    "}\n",
    "\n",
    "model_setup = ModelSetup(\n",
    "        name=\"forward_testing_model\",\n",
    "        use_clinical=False,\n",
    "        use_fixations=True,\n",
    "        spatialise_clinical=False,\n",
    "        add_clinical_to_roi_heads=False,\n",
    "        **mobilenet_args,\n",
    "        **small_model_args,\n",
    "        **common_args,\n",
    "        **fusion_add_args,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate datasets and dataloaders\n",
    "The batch size is also defined in this section. For testing purpose, we only set it as 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params_dict = {\n",
    "    \"XAMI_MIMIC_PATH\": XAMI_MIMIC_PATH,\n",
    "    \"with_clinical\": model_setup.use_clinical,\n",
    "    \"dataset_mode\": model_setup.dataset_mode,\n",
    "    \"bbox_to_mask\": model_setup.use_mask,\n",
    "    \"labels_cols\": DEFAULT_REFLACX_LABEL_COLS,\n",
    "}\n",
    "\n",
    "detect_eval_dataset, train_dataset, val_dataset, test_dataset = get_datasets(\n",
    "    dataset_params_dict=dataset_params_dict\n",
    ")\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = get_dataloaders(\n",
    "    train_dataset, val_dataset, test_dataset, batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We used to have 670, after unifying, we will have 590.\n"
     ]
    }
   ],
   "source": [
    "print(f\"We used to have {len(detect_eval_dataset.df.dicom_id)}, after unifying, we will have {len(detect_eval_dataset.df.dicom_id.unique())}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example instance from dataset\n",
    "We show what's inside a single instance. It will provide:\n",
    "\n",
    "- Images\n",
    "- Clinical data\n",
    "- Targets (Dictionary)\n",
    "\n",
    "And, inside the target, there're:\n",
    "\n",
    "- boxes (bounding boxes of abnormality)\n",
    "- lable (disease index (Note: the class **0** means the background))\n",
    "- image_id (idx to get that image)\n",
    "- area (the areas that bouding boxes contain)\n",
    "- iscrowd (if it's a place with multiple bouding boxes, we assume all the the bouding boxes are not crowd.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.5608, 0.5608, 0.5569,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5569, 0.5647, 0.5647,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5490, 0.5569, 0.5608,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.7686, 0.7686, 0.7647,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.7608, 0.7647, 0.7608,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.7529, 0.7569, 0.7569,  ..., 1.0000, 1.0000, 1.0000]],\n",
       " \n",
       "         [[0.5608, 0.5608, 0.5569,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5569, 0.5647, 0.5647,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5490, 0.5569, 0.5608,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.7686, 0.7686, 0.7647,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.7608, 0.7647, 0.7608,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.7529, 0.7569, 0.7569,  ..., 1.0000, 1.0000, 1.0000]],\n",
       " \n",
       "         [[0.5608, 0.5608, 0.5569,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5569, 0.5647, 0.5647,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5490, 0.5569, 0.5608,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.7686, 0.7686, 0.7647,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.7608, 0.7647, 0.7608,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.7529, 0.7569, 0.7569,  ..., 1.0000, 1.0000, 1.0000]]]),\n",
       " {'boxes': tensor([[ 734., 1204., 2211., 2175.]], dtype=torch.float64),\n",
       "  'labels': tensor([1]),\n",
       "  'image_id': tensor([0]),\n",
       "  'area': tensor([1434167.], dtype=torch.float64),\n",
       "  'iscrowd': tensor([0]),\n",
       "  'dicom_id': '34cedb74-d0996b40-6d218312-a9174bea-d48dc033',\n",
       "  'image_path': 'D:\\\\XAMI-MIMIC\\\\patient_18111516\\\\CXR-JPG\\\\s55032240\\\\34cedb74-d0996b40-6d218312-a9174bea-d48dc033.jpg',\n",
       "  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model.\n",
    "\n",
    "We define he models here. Two backbone examples are in the below code section. The MobileNet is a light weight network, and ResNet is heavier, but usually perform better. In our case, the calculation is not the most important factor; therefore, we chose ResNet with feature pyramid networks (FPN) backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load custom model\n",
      "Using pretrained backbone. mobilenet_v3\n",
      "Using pretrained backbone. mobilenet_v3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'fixations_backbone'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\UTILIZ~1\\AppData\\Local\\Temp/ipykernel_664/3991185099.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model = create_model_from_setup(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mlabels_cols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel_setup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mrpn_nms_thresh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mbox_detections_per_img\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Utilizador\\Desktop\\TESE\\multimodal-abnormalities-detection\\models\\build.py\u001b[0m in \u001b[0;36mcreate_model_from_setup\u001b[1;34m(labels_cols, setup, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_custom_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Load custom model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         model = get_multimodal_rcnn_model(\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0msetup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_cols\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\Utilizador\\Desktop\\TESE\\multimodal-abnormalities-detection\\models\\build.py\u001b[0m in \u001b[0;36mget_multimodal_rcnn_model\u001b[1;34m(num_classes, setup, mask_hidden_layers, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         model = multimodal_maskrcnn_with_backbone(\n\u001b[0m\u001b[0;32m     63\u001b[0m             \u001b[0msetup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpretrained_backbone\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\Utilizador\\Desktop\\TESE\\multimodal-abnormalities-detection\\models\\build.py\u001b[0m in \u001b[0;36mmultimodal_maskrcnn_with_backbone\u001b[1;34m(setup, pretrained_backbone, num_classes, **kwargs)\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[0mclinical_backbone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatialise_clinical\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatialise_method\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"convs\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m         \u001b[0mclinical_backbone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_clinical_backbone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m     model = MultimodalMaskRCNN(\n",
      "\u001b[1;32mc:\\Users\\Utilizador\\Desktop\\TESE\\multimodal-abnormalities-detection\\models\\detectors\\rcnn.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, setup, backbone, num_classes, min_size, max_size, image_mean, image_std, rpn_anchor_generator, rpn_head, rpn_pre_nms_top_n_train, rpn_pre_nms_top_n_test, rpn_post_nms_top_n_train, rpn_post_nms_top_n_test, rpn_nms_thresh, rpn_fg_iou_thresh, rpn_bg_iou_thresh, rpn_batch_size_per_image, rpn_positive_fraction, rpn_score_thresh, box_roi_pool, box_head, box_predictor, box_score_thresh, box_nms_thresh, box_detections_per_img, box_fg_iou_thresh, box_bg_iou_thresh, box_batch_size_per_image, box_positive_fraction, bbox_reg_weights, mask_roi_pool, mask_head, mask_predictor, clinical_backbone, fixations_backbone)\u001b[0m\n\u001b[0;32m   1681\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1682\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmask_predictor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1683\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m   1684\u001b[0m                     \u001b[1;34m\"num_classes should be None when mask_predictor is specified\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1685\u001b[0m                 )\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'fixations_backbone'"
     ]
    }
   ],
   "source": [
    "model = create_model_from_setup(\n",
    "    labels_cols,\n",
    "    model_setup,\n",
    "    rpn_nms_thresh=0.3,\n",
    "    box_detections_per_img=10,\n",
    "    box_nms_thresh=0.2,\n",
    "    rpn_score_thresh=0.0,\n",
    "    box_score_thresh=0.05,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data to feed\n",
    "\n",
    "We prepare three main data to test the model:\n",
    "\n",
    "- CXR image\n",
    "- Clinical data\n",
    "- Target\n",
    "\n",
    "And, for each data, we adjust the format to what the model expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_dataloader))\n",
    "data = train_dataset.prepare_input_from_data(data, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Feedforawrd (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "loss_dict, outputs = model(*data[:-1], targets=data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, clinical_num, clinical_cat, targets = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "original_image_sizes= []\n",
    "for img in images:\n",
    "    val = img.shape[-2:]\n",
    "    assert len(val) == 2\n",
    "    original_image_sizes.append((val[0], val[1]))\n",
    "\n",
    "images, targets = model.transform(images, targets)\n",
    "\n",
    "img_features = model.backbone(images.tensors)\n",
    "\n",
    "print(img_features.shape)\n",
    "\n",
    "# for k, v in img_features.items():\n",
    "#     print(f\"[{k}]: {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results we get.\n",
    "Four different losses are given in the result, we will use these losses to optimise the network while training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Feedforawrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection.\n",
    "\n",
    "A detection contain *boxes*, *lables*, and *scores*.\n",
    "\n",
    "- *boxes*: All the bounding boxes for this image. \n",
    "- *lables*: Labels corresponded to the bounding boxes.\n",
    "- *score*: Score (Confidence) for each boudning box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'loss_classifier': tensor(1.8703, grad_fn=<NllLossBackward0>),\n",
       "  'loss_box_reg': tensor(0.0083, grad_fn=<DivBackward0>),\n",
       "  'loss_mask': tensor(0.7661, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       "  'loss_objectness': tensor(0.6923, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       "  'loss_rpn_box_reg': tensor(0.0056, dtype=torch.float64, grad_fn=<DivBackward0>)},\n",
       " [{'boxes': tensor([[2.3629e+03, 1.0602e+02, 2.6215e+03, 2.2353e+02],\n",
       "           [7.2340e+02, 1.9290e+02, 1.2145e+03, 1.1568e+03],\n",
       "           [2.1753e+03, 8.1519e-01, 2.4317e+03, 5.7176e+01],\n",
       "           [2.3989e+03, 7.1049e-01, 2.5735e+03, 8.7468e+01],\n",
       "           [1.0604e+03, 8.0044e+01, 1.2399e+03, 2.5090e+02],\n",
       "           [1.6428e+03, 8.4712e+02, 1.7817e+03, 1.0633e+03],\n",
       "           [1.6446e+03, 1.0116e+03, 1.7859e+03, 1.2136e+03],\n",
       "           [1.0849e+03, 1.1581e+03, 1.2075e+03, 1.4033e+03],\n",
       "           [3.2304e+02, 1.7968e+03, 4.4830e+02, 2.0410e+03],\n",
       "           [2.5567e+03, 3.8252e-01, 2.8163e+03, 6.0379e+01]],\n",
       "          grad_fn=<StackBackward0>),\n",
       "   'labels': tensor([5, 5, 5, 5, 5, 2, 2, 5, 5, 5]),\n",
       "   'scores': tensor([0.3043, 0.3021, 0.2915, 0.2896, 0.2895, 0.2871, 0.2866, 0.2859, 0.2848,\n",
       "           0.2840], grad_fn=<IndexBackward0>),\n",
       "   'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<UnsqueezeBackward0>)},\n",
       "  {'boxes': tensor([[1762.7408,  632.2818, 2127.8401,  955.9152],\n",
       "           [1785.4111,  583.7678, 2052.9829,  696.5234],\n",
       "           [2513.8635,  950.7032, 2885.1704, 1283.8948],\n",
       "           [1365.2786, 1424.1830, 1737.2183, 1758.2413],\n",
       "           [2553.0557, 1220.3689, 2822.8767, 1332.4296],\n",
       "           [1409.9410, 1377.4469, 1677.2332, 1490.9060],\n",
       "           [ 732.4443,  500.3598, 1148.9506,  774.6163],\n",
       "           [ 607.4332,  593.2745,  909.6434,  684.9359],\n",
       "           [1592.4390, 1644.9053, 2193.1460, 1847.6277],\n",
       "           [ 413.9069, 1863.2333,  718.8145, 1955.5883]],\n",
       "          grad_fn=<StackBackward0>),\n",
       "   'labels': tensor([5, 5, 5, 5, 5, 5, 2, 2, 2, 2]),\n",
       "   'scores': tensor([0.2989, 0.2905, 0.2825, 0.2816, 0.2809, 0.2808, 0.2798, 0.2777, 0.2762,\n",
       "           0.2757], grad_fn=<IndexBackward0>),\n",
       "   'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<UnsqueezeBackward0>)},\n",
       "  {'boxes': tensor([[  16.2351, 1702.8088,  323.4251, 1798.6648],\n",
       "           [  21.1549,  752.6216,  328.6780,  847.9427],\n",
       "           [1007.2571, 1647.4432, 1612.1864, 1848.4729],\n",
       "           [  17.7368, 1545.7238,  324.9188, 1642.3929],\n",
       "           [1168.5548,  908.2289, 1485.1732, 1000.6577],\n",
       "           [   0.0000, 1804.3433,  456.9363, 2012.2211],\n",
       "           [  24.1183,  908.3090,  329.1682, 1005.0072],\n",
       "           [ 303.8716,  213.2775,  446.4726,  418.5415],\n",
       "           [   0.0000, 1393.1615,  543.1860, 2487.9902],\n",
       "           [2507.0383,  590.2036, 2813.7043,  687.1287]],\n",
       "          grad_fn=<StackBackward0>),\n",
       "   'labels': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       "   'scores': tensor([0.3018, 0.2987, 0.2964, 0.2957, 0.2948, 0.2938, 0.2931, 0.2920, 0.2909,\n",
       "           0.2904], grad_fn=<IndexBackward0>),\n",
       "   'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<UnsqueezeBackward0>)},\n",
       "  {'boxes': tensor([[1592.1448,  584.7238, 1863.3582,  697.8925],\n",
       "           [2128.8884,  586.4219, 2432.6482,  684.9879],\n",
       "           [ 882.0791, 1958.7018, 1021.6334, 2169.3020],\n",
       "           [ 833.4772, 1788.8124, 1398.3649, 2004.6130],\n",
       "           [ 995.3109,  425.1875, 1289.2811,  529.2990],\n",
       "           [1691.1465,  196.7693, 2190.5557,  449.1560],\n",
       "           [ 688.4447, 1327.1779,  830.4719, 1532.3887],\n",
       "           [  31.9612, 2181.0281,  327.6247, 2275.7061],\n",
       "           [1972.7262,  268.2437, 2243.3967,  376.6022],\n",
       "           [2026.9360,  685.4554, 2167.5791,  899.8092]],\n",
       "          grad_fn=<StackBackward0>),\n",
       "   'labels': tensor([5, 2, 2, 2, 2, 5, 2, 2, 5, 2]),\n",
       "   'scores': tensor([0.2906, 0.2873, 0.2810, 0.2810, 0.2805, 0.2803, 0.2784, 0.2780, 0.2770,\n",
       "           0.2765], grad_fn=<IndexBackward0>),\n",
       "   'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           ...,\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "   \n",
       "   \n",
       "           [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             ...,\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "             [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<UnsqueezeBackward0>)}])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_dict, outputs "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37134ace218921b74af13dd92c56fdadfb15c5b94d65c3fac1e4c7a849f19ee9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
